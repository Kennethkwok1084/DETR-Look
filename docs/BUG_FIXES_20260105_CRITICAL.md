# è®­ç»ƒåŸºæœ¬åŠŸå…³é”® Bug ä¿®å¤ (2026-01-05)

## ğŸ“‹ ä¿®å¤æ¦‚è§ˆ

æœ¬æ¬¡ä¿®å¤è§£å†³äº†è®­ç»ƒåŸºæœ¬åŠŸå®ç°ä¸­çš„**9ä¸ªå…³é”®é—®é¢˜**ï¼ŒåŒ…æ‹¬1ä¸ªå¯åŠ¨çº§é”™è¯¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰ã€3ä¸ªè¿è¡Œæ—¶å¼‚å¸¸ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰å’Œ5ä¸ªé…ç½®ä¼˜åŒ–ï¼ˆä½ä¼˜å…ˆçº§ï¼‰ã€‚

---

## ğŸ”´ é«˜ä¼˜å…ˆçº§ä¿®å¤

### 1. resume_checkpoint UnboundLocalError

**é—®é¢˜æè¿°**ï¼š
- `resume_checkpoint` åœ¨å®šä¹‰å‰è¢«å¼•ç”¨ï¼Œå¯¼è‡´ `UnboundLocalError`
- è®­ç»ƒè„šæœ¬æ— æ³•å¯åŠ¨

**é—®é¢˜ä»£ç **ï¼š
```python
# tools/train_detr.py (æ—§ç‰ˆ)
is_resume = bool(resume_checkpoint)  # ç¬¬230è¡Œï¼šä½¿ç”¨
metrics_logger = MetricsLogger(output_dir, resume=is_resume)

# ... 50è¡Œä¹‹å ...
resume_checkpoint = args.resume or config['training'].get('resume')  # ç¬¬278è¡Œï¼šå®šä¹‰
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# tools/train_detr.py (æ–°ç‰ˆ)
# Resume æ£€æŸ¥ï¼ˆåœ¨ä½¿ç”¨å‰å®šä¹‰ï¼‰
resume_checkpoint = args.resume or config['training'].get('resume')  # ç¬¬229è¡Œï¼šå®šä¹‰
is_resume = bool(resume_checkpoint)  # ç¬¬230è¡Œï¼šä½¿ç”¨

# è®¾ç½®æ—¥å¿—ï¼ˆResume æ¨¡å¼ç»­å†™ï¼‰
logger = setup_logger('train', output_dir / 'train.log')
metrics_logger = MetricsLogger(output_dir, resume=is_resume)
```

**éªŒè¯æ–¹æ³•**ï¼š
```bash
python tools/train_detr.py --config configs/detr_smoke.yaml --resume outputs/checkpoints/checkpoint_epoch_001.pth
```

---

## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ä¿®å¤

### 2. MetricsLogger.get_best() å¼•ç”¨å·²åˆ é™¤å˜é‡

**é—®é¢˜æè¿°**ï¼š
- `get_best()` æ–¹æ³•å¼•ç”¨ `self.metrics_history`ï¼Œä½†è¯¥å˜é‡å·²åœ¨å‰æœŸé‡æ„ä¸­åˆ é™¤
- è°ƒç”¨ `get_best()` ä¼šæŠ›å‡º `AttributeError`

**é—®é¢˜ä»£ç **ï¼š
```python
# utils/logger.py (æ—§ç‰ˆ)
def get_best(self, metric_name: str, mode: str = 'max') -> Dict[str, Any]:
    if not self.metrics_history:  # âŒ metrics_history å·²åˆ é™¤
        return {}
    
    if mode == 'max':
        best_record = max(self.metrics_history, key=lambda x: x.get(metric_name, float('-inf')))
    else:
        best_record = min(self.metrics_history, key=lambda x: x.get(metric_name, float('inf')))
    
    return best_record
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# utils/logger.py (æ–°ç‰ˆ)
def get_best(self, metric_name: str, mode: str = 'max') -> Optional[Dict[str, Any]]:
    if not self.metrics:  # âœ… ä½¿ç”¨ç»Ÿä¸€çš„ self.metrics
        return None
    
    # è¿‡æ»¤å‡ºåŒ…å«è¯¥æŒ‡æ ‡çš„è®°å½•
    valid_records = [r for r in self.metrics if metric_name in r]
    if not valid_records:
        return None
    
    if mode == 'max':
        return max(valid_records, key=lambda x: x[metric_name])
    else:
        return min(valid_records, key=lambda x: x[metric_name])
```

**å…³é”®å˜æ›´**ï¼š
- `self.metrics_history` â†’ `self.metrics`
- è¿”å› `None` è€Œéç©ºå­—å…¸ `{}`ï¼ˆæ›´ç¬¦åˆ Python æƒ¯ä¾‹ï¼‰
- æ·»åŠ  `valid_records` è¿‡æ»¤ï¼Œé¿å… `KeyError`

---

### 3. Resume æ¨¡å¼è¦†ç›– CSV å†å²è®°å½•

**é—®é¢˜æè¿°**ï¼š
- Resume æ—¶ `csv_header_written` åˆå§‹åŒ–ä¸º `False`
- ç¬¬ä¸€æ¡è®°å½•ä½¿ç”¨ `'w'` æ¨¡å¼å†™å…¥ï¼Œè¦†ç›–å†å² CSV

**é—®é¢˜ä»£ç **ï¼š
```python
# utils/logger.py (æ—§ç‰ˆ)
def __init__(self, output_dir: Path, resume: bool = False):
    # ... åŠ è½½ JSON ...
    
    self.csv_header_written = False  # âŒ Resume æ—¶ä¹Ÿæ˜¯ False
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# utils/logger.py (æ–°ç‰ˆ)
def __init__(self, output_dir: Path, resume: bool = False):
    # Resumeæ¨¡å¼ï¼šåŠ è½½å·²æœ‰æŒ‡æ ‡
    self.metrics = []
    if resume and self.json_path.exists():
        try:
            with open(self.json_path, 'r') as f:
                self.metrics = json.load(f)
            print(f"ğŸ“‚ Resume: å·²åŠ è½½ {len(self.metrics)} æ¡å†å²æŒ‡æ ‡")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½å†å²æŒ‡æ ‡: {e}ï¼Œä»ç©ºåˆ—è¡¨å¼€å§‹")
            self.metrics = []
    
    # CSV çŠ¶æ€ï¼šResume æ—¶æ£€æŸ¥æ˜¯å¦å·²æœ‰ CSV
    self.csv_header_written = False
    if resume and self.csv_path.exists():
        # å·²æœ‰ CSVï¼Œè®¾ç½®ä¸ºå·²å†™å…¥ headerï¼ˆåç»­ç”¨ append æ¨¡å¼ï¼‰
        self.csv_header_written = True
        print(f"ğŸ“‚ Resume: å°†ç»­å†™ CSV æ–‡ä»¶")
```

**å…³é”®é€»è¾‘**ï¼š
- Resume ä¸” CSV å­˜åœ¨ â†’ `csv_header_written = True`
- åç»­ `log()` ä½¿ç”¨ `'a'` æ¨¡å¼è¿½åŠ ï¼Œä¸è¦†ç›–

---

### 4. Resume æ—¶ JSON/CSV ä¸€è‡´æ€§æ£€æµ‹

**é—®é¢˜æè¿°**ï¼š
- Resume æ—¶å¦‚æœ JSON ä¸¢å¤±/æŸåä½† CSV ä»å­˜åœ¨
- JSON ä»ç©ºåˆ—è¡¨å¼€å§‹ï¼ŒCSV ç»§ç»­è¿½åŠ  â†’ ä¸¤è€…ä¸ä¸€è‡´

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# utils/logger.py (æ–°ç‰ˆ)
def __init__(self, output_dir: Path, resume: bool = False):
    # Resumeæ¨¡å¼ï¼šåŠ è½½å·²æœ‰æŒ‡æ ‡
    self.metrics = []
    json_loaded = False
    if resume and self.json_path.exists():
        try:
            with open(self.json_path, 'r') as f:
                self.metrics = json.load(f)
            json_loaded = True
            print(f"ğŸ“‚ Resume: å·²åŠ è½½ {len(self.metrics)} æ¡å†å²æŒ‡æ ‡")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½å†å²æŒ‡æ ‡: {e}ï¼Œä»ç©ºåˆ—è¡¨å¼€å§‹")
            self.metrics = []
    
    # CSV çŠ¶æ€ï¼šResume æ—¶æ£€æŸ¥æ˜¯å¦å·²æœ‰ CSV
    self.csv_header_written = False
    csv_exists = resume and self.csv_path.exists()
    if csv_exists:
        self.csv_header_written = True
        print(f"ğŸ“‚ Resume: å°†ç»­å†™ CSV æ–‡ä»¶")
    
    # ä¸€è‡´æ€§æ£€æŸ¥ï¼šResume æ—¶ CSV å­˜åœ¨ä½† JSON ä¸å­˜åœ¨ï¼ˆæˆ–åŠ è½½å¤±è´¥ï¼‰
    if resume and csv_exists and not json_loaded:
        print(f"âš ï¸  è­¦å‘Š: CSV å­˜åœ¨ä½† JSON ç¼ºå¤±/æŸå")
        print(f"    â†’ CSV å°†ç»§ç»­è¿½åŠ ï¼Œä½†å†å²æŒ‡æ ‡æ— æ³•åœ¨ JSON ä¸­ä½“ç°")
        print(f"    â†’ å»ºè®®æ£€æŸ¥ {self.json_path} æˆ–æ‰‹åŠ¨æ¢å¤")
```

**å…³é”®æ”¹è¿›**ï¼š
- å¢åŠ  `json_loaded` æ ‡å¿—è¿½è¸ª JSON åŠ è½½çŠ¶æ€
- æ£€æµ‹ `csv_exists and not json_loaded` æƒ…å†µ
- è¾“å‡ºè­¦å‘Šæç¤ºç”¨æˆ·æ•°æ®ä¸ä¸€è‡´é£é™©

---

## ğŸŸ¢ ä½ä¼˜å…ˆçº§ä¼˜åŒ–

### 5. Progressive Resizing longest_edge é…ç½®åŒ–

**é—®é¢˜æè¿°**ï¼š
- åŸå®ç°å¼ºåˆ¶ `longest_edge = shortest_edge`ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦å‹ç¼©
- æ— æ³•ä¿ç•™ Deformable DETR é»˜è®¤çš„é•¿è¾¹ä¸Šé™ 1333

**ä¼˜åŒ–å‰**ï¼š
```python
# tools/train_detr.py (æ—§ç‰ˆ)
image_processor.size = {
    "shortest_edge": current_size,
    "longest_edge": current_size  # âŒ å¼ºåˆ¶ç­‰è¾¹
}
```

**ä¼˜åŒ–å**ï¼š
```python
# tools/train_detr.py (æ–°ç‰ˆ)
if isinstance(current_size, dict):
    # å­—å…¸æ ¼å¼ï¼š{"shortest": 640, "longest": 1333}
    shortest = current_size.get('shortest', 800)
    longest = current_size.get('longest', 1333)
else:
    # æ•´æ•°æ ¼å¼ï¼šçŸ­è¾¹ä¸ºè¯¥å€¼ï¼Œé•¿è¾¹ä½¿ç”¨é»˜è®¤ä¸Šé™
    shortest = current_size
    longest = 1333  # Deformable DETR é»˜è®¤ä¸Šé™

image_processor.size = {"shortest_edge": shortest, "longest_edge": longest}
```

**é…ç½®ç¤ºä¾‹**ï¼š
```yaml
# configs/detr_baseline.yaml
training:
  resize_schedule:
    # æ•´æ•°æ ¼å¼ï¼ˆæ¨èï¼‰
    - [0, 640]   # çŸ­è¾¹640ï¼Œé•¿è¾¹1333
    - [10, 800]  # çŸ­è¾¹800ï¼Œé•¿è¾¹1333
    
    # æˆ–å­—å…¸æ ¼å¼ï¼ˆç²¾ç»†æ§åˆ¶ï¼‰
    - [0, {"shortest": 640, "longest": 1024}]
    - [10, {"shortest": 800, "longest": 1333}]
```

---

### 5. subset_filter_empty å¯é…ç½®

**é—®é¢˜æè¿°**ï¼š
- åŸå®ç°ä¸€å¾‹è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬
- å¯¼è‡´å­é›†åˆ†å¸ƒåç¦»å…¨é‡æ•°æ®ï¼ˆç©ºæ ‡æ³¨è¢«ç§»é™¤ï¼‰

**ä¼˜åŒ–å‰**ï¼š
```python
# dataset/coco_dataset.py (æ—§ç‰ˆ)
if subset_size:
    # ç­›é€‰æœ‰æ ‡æ³¨çš„æ ·æœ¬ï¼ˆè¿‡æ‹Ÿåˆæµ‹è¯•å¿…é¡»æœ‰æ ‡æ³¨ï¼‰
    valid_indices = [...]  # âŒ æ€»æ˜¯è¿‡æ»¤
    indices = random.sample(valid_indices, subset_size)
```

**ä¼˜åŒ–å**ï¼š
```python
# dataset/coco_dataset.py (æ–°ç‰ˆ)
if subset_size and image_set == 'train':
    # æ˜¯å¦è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬ï¼ˆé»˜è®¤ä»…è¿‡æ‹Ÿåˆæ¨¡å¼ä¸‹è¿‡æ»¤ï¼‰
    filter_empty = config['training'].get('subset_filter_empty', overfit_mode)
    
    if filter_empty:
        # ç­›é€‰æœ‰æ ‡æ³¨çš„æ ·æœ¬
        pool_indices = [idx for idx in range(len(dataset)) 
                       if dataset[idx][1].get('annotations')]
        print(f"ğŸ” å·²è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬ï¼š{len(dataset)} â†’ {len(pool_indices)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    else:
        # ä¸è¿‡æ»¤ï¼Œä¿æŒåŸå§‹åˆ†å¸ƒ
        pool_indices = list(range(len(dataset)))
        print(f"ğŸ“Š ä½¿ç”¨å…¨é‡æ ·æœ¬æ± ï¼ˆåŒ…å«ç©ºæ ‡æ³¨ï¼‰ï¼š{len(pool_indices)} ä¸ªæ ·æœ¬")
```

**é…ç½®è¯´æ˜**ï¼š
- `subset_filter_empty` æœªè®¾ç½® â†’ é»˜è®¤ `overfit_mode`ï¼ˆè¿‡æ‹Ÿåˆè¿‡æ»¤ï¼Œå¸¸è§„ä¸è¿‡æ»¤ï¼‰
- `subset_filter_empty: true` â†’ å¼ºåˆ¶è¿‡æ»¤ç©ºæ ‡æ³¨
- `subset_filter_empty: false` â†’ å¼ºåˆ¶ä¿ç•™ç©ºæ ‡æ³¨

---

### 7. subset_filter_empty é…ç½®æš´éœ²

**é—®é¢˜æè¿°**ï¼š
- ä»£ç ä¸­å·²å®ç° `subset_filter_empty` é€»è¾‘
- ä½† YAML é…ç½®æ–‡ä»¶æœªæš´éœ²è¯¥é€‰é¡¹

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```yaml
# configs/detr_baseline.yaml å’Œ detr_smoke.yaml
training:
  subset_size: null
  subset_seed: 42
  subset_filter_empty: null  # æ–°å¢ï¼šæ˜¯å¦è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬
                             # null=è‡ªåŠ¨ï¼Œtrue=å¼ºåˆ¶è¿‡æ»¤ï¼Œfalse=ä¿æŒåŸå§‹åˆ†å¸ƒ
                             # é»˜è®¤ï¼šoverfit æ¨¡å¼è¿‡æ»¤ï¼Œå¸¸è§„æ¨¡å¼ä¸è¿‡æ»¤
  overfit: false
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```yaml
# å¼ºåˆ¶æ‰€æœ‰æ ·æœ¬æœ‰æ ‡æ³¨ï¼ˆå³ä½¿é overfit æ¨¡å¼ï¼‰
training:
  subset_size: 100
  subset_filter_empty: true  # å¼ºåˆ¶è¿‡æ»¤
  overfit: false
```

---

### 8. overfit æ˜¾å¼ç¦ç”¨ transforms

**é—®é¢˜æè¿°**ï¼š
- å½“å‰ `make_transforms` è¿”å› `None`ï¼Œä½†æœªæ¥å¯ç”¨æ—¶éœ€è¦ç¡®ä¿è¿‡æ‹Ÿåˆæ¨¡å¼ä¸‹ç¦ç”¨
- ç¼ºå°‘æ˜¾å¼æ£€æŸ¥å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆç»“æœä¸ç¨³å®š

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# dataset/coco_dataset.py (æ–°ç‰ˆ)
if overfit_mode:
    shuffle = False
    # æ˜¾å¼ç¦ç”¨ transformsï¼ˆå½“å‰ make_transforms è¿”å› Noneï¼Œä½†ä½œä¸ºé˜²å¾¡æ€§æ£€æŸ¥ï¼‰
    if transforms is not None:
        print("âš ï¸  è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šå¼ºåˆ¶ç¦ç”¨ transforms ä»¥ç¡®ä¿å¯å¤ç°æ€§")
        transforms = None
    print(f"ğŸ“Œ è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šå…³é—­æ•°æ®å¢å¼ºå’Œæ‰“ä¹±")
```

**å…³é”®æ”¹è¿›**ï¼š
- å³ä½¿æœªæ¥å¯ç”¨ transformsï¼Œè¿‡æ‹Ÿåˆæ¨¡å¼ä¹Ÿä¼šå¼ºåˆ¶ç¦ç”¨
- ä¿è¯è¿‡æ‹Ÿåˆæµ‹è¯•çš„å¯å¤ç°æ€§

---

### 9. early_stopped æœªå†™å…¥ CSV

**é—®é¢˜æè¿°**ï¼š
- `run_trials.py` è®°å½•äº† `early_stopped` æ ‡è®°
- ä½† `save_results()` æœªå°†è¯¥å­—æ®µå†™å…¥ CSV

**ä¼˜åŒ–å‰**ï¼š
```python
# tools/run_trials.py (æ—§ç‰ˆ)
fieldnames = ['trial_id', 'status', 'final_map', 'final_loss', 'output_dir']
# âŒ ç¼ºå°‘ early_stopped
```

**ä¼˜åŒ–å**ï¼š
```python
# tools/run_trials.py (æ–°ç‰ˆ)
fieldnames = ['trial_id', 'status', 'final_map', 'final_loss', 'early_stopped', 'output_dir']

for result in results:
    row = {
        'trial_id': result['trial_id'],
        'status': result['status'],
        'final_map': result['final_map'],
        'final_loss': result['final_loss'],
        'early_stopped': result.get('early_stopped', False),  # âœ… å†™å…¥å­—æ®µ
        'output_dir': result['output_dir'],
    }
```

---

## âœ… éªŒè¯æ¸…å•

### è¯­æ³•æ£€æŸ¥
```bash
python tools/syntax_check.py
# âœ“ æ‰€æœ‰æ–‡ä»¶é€šè¿‡
```

### åŠŸèƒ½éªŒè¯

1. **Resume å¯åŠ¨**ï¼š
```bash
python tools/train_detr.py --config configs/detr_smoke.yaml --resume outputs/checkpoints/checkpoint.pth
# é¢„æœŸï¼šä¸æŠ› UnboundLocalError
```

2. **MetricsLogger.get_best()**ï¼š
```bash
python -c "
from utils.logger import MetricsLogger
from pathlib import Path
logger = MetricsLogger(Path('outputs'))
logger.log({'epoch': 1, 'mAP': 0.5})
best = logger.get_best('mAP', mode='max')
print(best)  # é¢„æœŸï¼š{'epoch': 1, 'mAP': 0.5}
"
```

3. **Resume CSV ç»­å†™**ï¼š
```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œ
python tools/train_detr.py --config configs/detr_smoke.yaml --max-epochs 2

# Resume è¿è¡Œ
python tools/train_detr.py --config configs/detr_smoke.yaml --resume outputs/checkpoints/checkpoint_epoch_002.pth --max-epochs 4

# æ£€æŸ¥ CSV
head -20 outputs/metrics.csv  # é¢„æœŸï¼šepoch 1-4 è¿ç»­ï¼Œæ— é‡å¤ header
```

4. **Progressive Resizing é…ç½®**ï¼š
```bash
# æµ‹è¯•å­—å…¸æ ¼å¼
python -c "
import yaml
config = yaml.safe_load('''
training:
  resize_schedule:
    - [0, {\"shortest\": 640, \"longest\": 1024}]
    - [5, {\"shortest\": 800, \"longest\": 1333}]
''')
print(config['training']['resize_schedule'])
"
```

5. **subset_filter_empty**ï¼š
```bash
# è¿‡æ‹Ÿåˆæ¨¡å¼ï¼ˆé»˜è®¤è¿‡æ»¤ï¼‰
python tools/train_detr.py --config configs/detr_smoke.yaml --subset-size 10 --overfit

# å¸¸è§„æ¨¡å¼ï¼ˆé»˜è®¤ä¸è¿‡æ»¤ï¼‰
python tools/train_detr.py --config configs/detr_smoke.yaml --subset-size 100
```

6. **early_stopped CSV**ï¼š
```bash
python tools/run_trials.py --trials-file experiments/trials.json
cat outputs/trial_results.csv | head -1
# é¢„æœŸï¼štrial_id,status,final_map,final_loss,early_stopped,output_dir,...
```

7. **JSON/CSV ä¸€è‡´æ€§è­¦å‘Š**ï¼š
```bash
# æ¨¡æ‹Ÿåœºæ™¯ï¼šåˆ é™¤ JSON ä½†ä¿ç•™ CSV
rm outputs/metrics.json
python tools/train_detr.py --config configs/detr_smoke.yaml --resume outputs/checkpoints/checkpoint.pth

# é¢„æœŸè¾“å‡ºï¼š
# âš ï¸  è­¦å‘Š: CSV å­˜åœ¨ä½† JSON ç¼ºå¤±/æŸå
# â†’ CSV å°†ç»§ç»­è¿½åŠ ï¼Œä½†å†å²æŒ‡æ ‡æ— æ³•åœ¨ JSON ä¸­ä½“ç°
```

8. **subset_filter_empty é…ç½®**ï¼š
```bash
# æµ‹è¯•é…ç½®å¯è¯»å–
python -c "
import yaml
config = yaml.safe_load(open('configs/detr_smoke.yaml'))
print(config['training'].get('subset_filter_empty'))  # é¢„æœŸï¼šNone
"
```

9. **overfit transforms æ£€æŸ¥**ï¼š
```bash
# å½“å‰ transforms=Noneï¼Œä¸ä¼šè§¦å‘è­¦å‘Š
# æœªæ¥å¯ç”¨ transforms åï¼Œoverfit æ¨¡å¼ä¼šå¼ºåˆ¶ç¦ç”¨
python tools/train_detr.py --config configs/detr_smoke.yaml --subset-size 10 --overfit
# é¢„æœŸï¼šğŸ“Œ è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šå…³é—­æ•°æ®å¢å¼ºå’Œæ‰“ä¹±
```

---

## ğŸ“ ä¿®å¤æ€»ç»“

| ä¼˜å…ˆçº§ | Bug | æ–‡ä»¶ | çŠ¶æ€ |
|-------|-----|------|------|
| ğŸ”´ é«˜ | resume_checkpoint UnboundLocalError | train_detr.py | âœ… å·²ä¿®å¤ |
| ğŸŸ¡ ä¸­ | get_best() å¼•ç”¨ metrics_history | logger.py | âœ… å·²ä¿®å¤ |
| ğŸŸ¡ ä¸­ | Resume è¦†ç›– CSV | logger.py | âœ… å·²ä¿®å¤ |
| ï¿½ ä¸­ | Resume JSON/CSV ä¸€è‡´æ€§æ£€æµ‹ | logger.py | âœ… å·²ä¿®å¤ |
| ğŸŸ¢ ä½ | Progressive Resizing é…ç½®åŒ– | train_detr.py | âœ… å·²ä¼˜åŒ– |
| ğŸŸ¢ ä½ | subset_filter_empty å¯é…ç½® | coco_dataset.py | âœ… å·²ä¼˜åŒ– |
| ğŸŸ¢ ä½ | subset_filter_empty é…ç½®æš´éœ² | detr_*.yaml | âœ… å·²ä¼˜åŒ– |
| ğŸŸ¢ ä½ | overfit æ˜¾å¼ç¦ç”¨ transforms | coco_dataset.py | âœ… å·²ä¼˜åŒ– |
| ğŸŸ¢ ä½ | early_stopped CSV å­—æ®µ | run_trials.py | âœ… å·²ä¼˜åŒ– |

**ä¿®å¤ç»Ÿè®¡**ï¼š
- é«˜ä¼˜å…ˆçº§ï¼ˆå¯åŠ¨çº§ï¼‰ï¼š1 ä¸ª
- ä¸­ä¼˜å…ˆçº§ï¼ˆè¿è¡Œæ—¶ï¼‰ï¼š3 ä¸ª
- ä½ä¼˜å…ˆçº§ï¼ˆé…ç½®ä¼˜åŒ–ï¼‰ï¼š5 ä¸ª
- **æ€»è®¡**ï¼š9 ä¸ª

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… **è¯­æ³•éªŒè¯** - å·²é€šè¿‡
2. â­ï¸ **å†’çƒŸæµ‹è¯•** - `python tools/train_detr.py --config configs/detr_smoke.yaml`
3. â­ï¸ **Resume æµ‹è¯•** - ä¸­æ–­åæ·»åŠ  `--resume` éªŒè¯ç»­å†™
4. â­ï¸ **è¿‡æ‹Ÿåˆæµ‹è¯•** - `--subset-size 10 --overfit` éªŒè¯ RNG å’Œæ ·æœ¬è¿‡æ»¤
5. â­ï¸ **Progressive Resizing æµ‹è¯•** - é…ç½®ä¸åŒå°ºå¯¸éªŒè¯ processor æ›´æ–°
6. â­ï¸ **Trial æœç´¢æµ‹è¯•** - éªŒè¯ early_stopped æ ‡è®°å†™å…¥ CSV

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è®­ç»ƒåŸºæœ¬åŠŸæŒ‡å—](TRAINING_GUIDE.md)
- [å…³é”®ä¿®å¤ (2026-01-05)](CRITICAL_FIXES_20260105.md)
- [å¼€å‘æŒ‡å—](develop.md)
