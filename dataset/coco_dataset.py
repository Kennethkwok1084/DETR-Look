#!/usr/bin/env python3
"""
COCOæ ¼å¼æ•°æ®é›†åŠ è½½å™¨
æ”¯æŒDETRè®­ç»ƒæ‰€éœ€çš„æ•°æ®å¢å¼ºå’Œæ ¼å¼è½¬æ¢
"""

import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
import torchvision.transforms as T
from PIL import Image
from pycocotools.coco import COCO
from torch.utils.data import DataLoader, Dataset


class CocoDetectionDataset(Dataset):
    """
    COCOæ ¼å¼ç›®æ ‡æ£€æµ‹æ•°æ®é›†
    
    è¿”å›æ ¼å¼ç¬¦åˆDETRè¦æ±‚ï¼š
    - image: [3, H, W] çš„ tensor
    - target: dictåŒ…å« 'boxes', 'labels', 'image_id' ç­‰
    """
    
    def __init__(
        self,
        img_folder: str,
        ann_file: str,
        transforms: Optional[Any] = None,
        return_masks: bool = False,
    ):
        """
        Args:
            img_folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            ann_file: COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            transforms: æ•°æ®å¢å¼ºpipeline
            return_masks: æ˜¯å¦è¿”å›åˆ†å‰²maskï¼ˆæœ¬é¡¹ç›®ä¸éœ€è¦ï¼‰
        """
        self.img_folder = Path(img_folder)
        self.coco = COCO(ann_file)
        self.ids = list(sorted(self.coco.imgs.keys()))
        self.transforms = transforms
        self.return_masks = return_masks
        
    def __len__(self) -> int:
        return len(self.ids)
    
    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:
        """
        è¿”å›å•ä¸ªæ ·æœ¬ï¼ˆåŸå§‹æ ¼å¼ï¼Œä¾›DetrImageProcessorå¤„ç†ï¼‰
        
        Returns:
            image: PIL.Image (RGBï¼Œæœªå½’ä¸€åŒ–)
            target: COCOæ ¼å¼æ ‡æ³¨å­—å…¸ {
                'image_id': int,
                'annotations': List[{
                    'bbox': [x, y, w, h],  # COCOæ ¼å¼ï¼šxywhåƒç´ åæ ‡
                    'category_id': int,
                    'area': float,
                    'iscrowd': int,
                }]
            }
        """
        img_id = self.ids[idx]
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)
        
        # åŠ è½½å›¾åƒï¼ˆPILæ ¼å¼ï¼Œä¸è½¬tensorï¼‰
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = self.img_folder / img_info['file_name']
        image = Image.open(img_path).convert('RGB')
        
        # æ„å»ºCOCOæ ¼å¼æ ‡æ³¨ï¼ˆDetrImageProcessoræœŸæœ›çš„æ ¼å¼ï¼‰
        annotations = []
        for ann in anns:
            annotations.append({
                'bbox': ann['bbox'],  # ä¿æŒCOCOçš„[x, y, w, h]æ ¼å¼
                'category_id': ann['category_id'],
                'area': ann['area'],
                'iscrowd': ann.get('iscrowd', 0),
            })
        
        target = {
            'image_id': img_id,
            'annotations': annotations,
        }
        
        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚æœæä¾›ï¼‰
        # âš ï¸  è­¦å‘Šï¼šå½“å‰å®ç°ä»…å¯¹å›¾åƒåº”ç”¨å¢å¼ºï¼Œbbox ä¸ä¼šåŒæ­¥å˜æ¢
        # å¦‚éœ€å‡ ä½•å˜æ¢ï¼ˆç¿»è½¬/è£å‰ª/æ—‹è½¬ï¼‰ï¼Œè¯·ä½¿ç”¨æ”¯æŒ bbox å˜æ¢çš„åº“å¦‚ albumentations
        # æˆ–ç¡®ä¿ transforms ä»…åŒ…å«é¢œè‰²å¢å¼ºï¼ˆColorJitter ç­‰ï¼‰
        if self.transforms is not None:
            import warnings
            warnings.warn(
                "å½“å‰æ•°æ®å¢å¼ºä»…ä½œç”¨äºå›¾åƒï¼Œbbox ä¸ä¼šåŒæ­¥å˜æ¢ã€‚"
                "å‡ ä½•å˜æ¢ï¼ˆç¿»è½¬/è£å‰ªï¼‰ä¼šå¯¼è‡´æ ‡æ³¨é”™ä½ã€‚"
                "å»ºè®®ä½¿ç”¨ albumentations æˆ–ä»…ä½¿ç”¨é¢œè‰²å¢å¼ºã€‚",
                UserWarning
            )
            image = self.transforms(image)
        
        return image, target


def make_transforms(image_set: str, config: dict) -> Any:
    """
    æ„å»ºæ•°æ®å¢å¼ºpipeline
    
    æ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬ç”±DetrImageProcessorç»Ÿä¸€å¤„ç†resize/pad/normalizeï¼Œ
    å› æ­¤è¿”å›Noneã€‚å¦‚éœ€é¢å¤–æ•°æ®å¢å¼ºï¼ˆå¦‚RandomHorizontalFlipã€ColorJitterï¼‰ï¼Œ
    å¯åœ¨æ­¤æ„å»ºtorchvision.transforms.Composeå¹¶åœ¨__getitem__ä¸­å¯¹PILå›¾åƒåº”ç”¨ã€‚
    
    å‚è€ƒå®ç°ï¼š
    if image_set == 'train':
        return transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
        ])
    
    Args:
        image_set: 'train' æˆ– 'val'
        config: é…ç½®å­—å…¸
    
    Returns:
        None (å½“å‰ä¸ä½¿ç”¨transformsï¼Œç›´æ¥è¿”å›PILå›¾åƒ)
    """
    # DetrImageProcessorä¼šè‡ªåŠ¨å¤„ç†resize/pad/normalize
    # å¦‚éœ€é¢å¤–å¢å¼ºå¯åœ¨æ­¤æ·»åŠ ï¼Œåœ¨__getitem__ä¸­åº”ç”¨åˆ°PILå›¾åƒåå†ä¼ ç»™processor
    return None


def collate_fn(batch: List[Tuple[Image.Image, Dict]]) -> Tuple[List[Image.Image], List[Dict]]:
    """
    è‡ªå®šä¹‰collateå‡½æ•°ï¼Œè¿”å›åŸå§‹PILå›¾åƒå’ŒCOCOæ ‡æ³¨
    ä¾›DetrImageProcessoræ‰¹é‡å¤„ç†
    
    Args:
        batch: [(PIL.Image, target_dict), ...] åˆ—è¡¨
    
    Returns:
        images: List[PIL.Image]
        targets: List[Dict] - COCOæ ¼å¼æ ‡æ³¨
    """
    images, targets = zip(*batch)
    return list(images), list(targets)


def build_dataloader(
    config: dict,
    image_set: str,
    batch_size: Optional[int] = None,
    num_workers: Optional[int] = None,
    shuffle: Optional[bool] = None,
) -> DataLoader:
    """
    æ„å»ºDataLoaderï¼Œæ”¯æŒå­é›†é‡‡æ ·å’Œè¿‡æ‹Ÿåˆæ¨¡å¼
    
    Args:
        config: é…ç½®å­—å…¸
        image_set: 'train' æˆ– 'val'
        batch_size: æ‰¹å¤§å°ï¼ˆNoneåˆ™ä»configè¯»å–ï¼‰
        num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ˆNoneåˆ™ä»configè¯»å–ï¼‰
        shuffle: æ˜¯å¦æ‰“ä¹±ï¼ˆNoneåˆ™train=True, val=Falseï¼‰
    
    Returns:
        DataLoader
    """
    import random
    from torch.utils.data import Subset
    
    # ç¡®å®šå‚æ•°
    if batch_size is None:
        batch_size = config['training']['batch_size'] if image_set == 'train' else config['validation']['batch_size']
    
    if num_workers is None:
        num_workers = config['training']['num_workers'] if image_set == 'train' else config['validation']['num_workers']
    
    if shuffle is None:
        shuffle = (image_set == 'train')
    
    # æ„å»ºæ•°æ®é›†
    root = Path(config['dataset']['root_dir'])
    ann_file = root / config['dataset'][f'{image_set}_ann']
    img_folder = root / 'images' / image_set
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¿‡æ‹Ÿåˆæ¨¡å¼ï¼ˆéœ€è¦åœ¨ make_transforms ä¹‹å‰æ£€æŸ¥ï¼‰
    overfit_mode = config['training'].get('overfit', False)
    
    # æ„å»º transformsï¼ˆè¿‡æ‹Ÿåˆæ¨¡å¼ä¸‹å¼ºåˆ¶ä¸º Noneï¼‰
    if overfit_mode and image_set == 'train':
        transforms = None
        print("ğŸ“Œ è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šç¦ç”¨æ•°æ®å¢å¼ºï¼ˆtransforms=Noneï¼‰")
    else:
        transforms = make_transforms(image_set, config)
    
    dataset = CocoDetectionDataset(
        img_folder=str(img_folder),
        ann_file=str(ann_file),
        transforms=transforms,
    )
    
    # å­é›†é‡‡æ ·é€»è¾‘ï¼ˆç”¨äºå¿«é€ŸéªŒè¯æˆ–å°æ ·æœ¬è¿‡æ‹Ÿåˆï¼‰
    subset_size = config['training'].get('subset_size')
    
    if subset_size and image_set == 'train':
        # å›ºå®šéšæœºç§å­ä»¥ä¿è¯å­é›†å¯å¤ç°
        subset_seed = config['training'].get('subset_seed', 42)
        random.seed(subset_seed)
        
        # æ˜¯å¦è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬ï¼ˆé»˜è®¤ä»…è¿‡æ‹Ÿåˆæ¨¡å¼ä¸‹è¿‡æ»¤ï¼‰
        # filter_empty: True=å¼ºåˆ¶æœ‰æ ‡æ³¨, False=å…è®¸ç©ºæ ‡æ³¨ï¼ˆä¿æŒåŸå§‹åˆ†å¸ƒï¼‰
        filter_empty = config['training'].get('subset_filter_empty', overfit_mode)
        
        if filter_empty:
            # ç­›é€‰æœ‰æ ‡æ³¨çš„æ ·æœ¬ï¼ˆè¿‡æ‹Ÿåˆæµ‹è¯•å¿…é¡»æœ‰æ ‡æ³¨ï¼‰
            valid_indices = []
            for idx in range(len(dataset)):
                _, target = dataset[idx]
                if target.get('annotations') and len(target['annotations']) > 0:
                    valid_indices.append(idx)
            
            if len(valid_indices) == 0:
                raise ValueError(f"æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ ‡æ³¨çš„æ ·æœ¬ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
            
            pool_indices = valid_indices
            print(f"ğŸ” å·²è¿‡æ»¤ç©ºæ ‡æ³¨æ ·æœ¬ï¼š{len(dataset)} â†’ {len(pool_indices)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        else:
            # ä¸è¿‡æ»¤ï¼Œä½¿ç”¨å…¨é‡æ ·æœ¬æ± ï¼ˆä¿æŒåŸå§‹åˆ†å¸ƒï¼‰
            pool_indices = list(range(len(dataset)))
            print(f"ğŸ“Š ä½¿ç”¨å…¨é‡æ ·æœ¬æ± ï¼ˆåŒ…å«ç©ºæ ‡æ³¨ï¼‰ï¼š{len(pool_indices)} ä¸ªæ ·æœ¬")
        
        # éšæœºé€‰æ‹©æˆ–é¡ºåºé€‰æ‹©
        if overfit_mode:
            # è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šé€‰æ‹©å‰Nä¸ªæ ·æœ¬
            indices = pool_indices[:min(subset_size, len(pool_indices))]
            print(f"ğŸ“Œ è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šä» {len(pool_indices)} ä¸ªæ ·æœ¬ä¸­é€‰æ‹©å‰ {len(indices)} ä¸ªï¼ˆå›ºå®šç§å­={subset_seed}ï¼‰")
        else:
            # æ­£å¸¸å­é›†æ¨¡å¼ï¼šéšæœºé‡‡æ ·
            sample_size = min(subset_size, len(pool_indices))
            indices = random.sample(pool_indices, sample_size)
            print(f"ğŸ² å­é›†é‡‡æ ·ï¼šä» {len(pool_indices)} ä¸ªæ ·æœ¬ä¸­éšæœºé€‰æ‹© {len(indices)} ä¸ªï¼ˆç§å­={subset_seed}ï¼‰")
        
        dataset = Subset(dataset, indices)
        
        # è¿‡æ‹Ÿåˆæ¨¡å¼ä¸‹å¼ºåˆ¶ä¸æ‰“ä¹±
        if overfit_mode:
            shuffle = False
            print(f"ğŸ“Œ è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šå…³é—­æ‰“ä¹±ï¼ˆtransforms å·²åœ¨å‰é¢ç¦ç”¨ï¼‰")
    
    # æ„å»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    print(f"âœ… {image_set.upper()} DataLoader åˆ›å»ºæˆåŠŸ:")
    print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   Batch Size: {batch_size}")
    print(f"   Workers: {num_workers}")
    print(f"   Shuffle: {shuffle}")
    
    return dataloader
