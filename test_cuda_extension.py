#!/usr/bin/env python3
"""
æµ‹è¯• Deformable DETR CUDA æ‰©å±•æ˜¯å¦æ­£ç¡®ç¼–è¯‘å’Œå®‰è£…
"""

import sys
import torch

print("="*60)
print("æµ‹è¯• Deformable DETR CUDA æ‰©å±•")
print("="*60)

# æµ‹è¯• 1: æ£€æŸ¥ PyTorch CUDA å¯ç”¨æ€§
print("\n1. PyTorch CUDA çŠ¶æ€:")
print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(0)}")
else:
    print("   âš ï¸  CUDA ä¸å¯ç”¨ï¼ŒDeformable DETR æ— æ³•è¿è¡Œ")
    sys.exit(1)

# æµ‹è¯• 2: å¯¼å…¥ç¼–è¯‘çš„æ‰©å±•
print("\n2. å¯¼å…¥ CUDA æ‰©å±•:")
try:
    import MultiScaleDeformableAttention as MSDA
    print("   âœ… MultiScaleDeformableAttention æ¨¡å—å¯¼å…¥æˆåŠŸ")
    
    # æ£€æŸ¥å‡½æ•°
    if hasattr(MSDA, 'ms_deform_attn_forward'):
        print("   âœ… ms_deform_attn_forward å‡½æ•°å­˜åœ¨")
    if hasattr(MSDA, 'ms_deform_attn_backward'):
        print("   âœ… ms_deform_attn_backward å‡½æ•°å­˜åœ¨")
        
except ImportError as e:
    print(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("\n   è¯·ç¡®ä¿å·²ç¼–è¯‘ CUDA æ‰©å±•:")
    print("   cd third_party/deformable_detr/models/ops")
    print("   python setup.py build install")
    sys.exit(1)

# æµ‹è¯• 3: å¯¼å…¥ Python åŒ…è£…å™¨
print("\n3. å¯¼å…¥ Python åŒ…è£…å™¨:")
try:
    # æ·»åŠ è·¯å¾„
    from pathlib import Path
    third_party_path = Path(__file__).parent / "third_party" / "deformable_detr"
    sys.path.insert(0, str(third_party_path))
    
    from models.ops.modules import MSDeformAttn
    print("   âœ… MSDeformAttn æ¨¡å—å¯¼å…¥æˆåŠŸ")
    
except ImportError as e:
    print(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 4: åˆ›å»ºæ¨¡å—å®ä¾‹
print("\n4. åˆ›å»ºæ¨¡å—å®ä¾‹:")
try:
    d_model = 256
    n_levels = 4
    n_heads = 8
    n_points = 4
    
    msda = MSDeformAttn(
        d_model=d_model,
        n_levels=n_levels,
        n_heads=n_heads,
        n_points=n_points
    )
    msda = msda.cuda()
    
    print(f"   âœ… MSDeformAttn å®ä¾‹åˆ›å»ºæˆåŠŸ")
    print(f"      - d_model: {d_model}")
    print(f"      - n_levels: {n_levels}")
    print(f"      - n_heads: {n_heads}")
    print(f"      - n_points: {n_points}")
    
except Exception as e:
    print(f"   âŒ åˆ›å»ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯• 5: å‰å‘ä¼ æ’­æµ‹è¯•
print("\n5. å‰å‘ä¼ æ’­æµ‹è¯•:")
try:
    batch_size = 2
    num_queries = 300
    
    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    # input_spatial_shapes å®šä¹‰æ¯ä¸ªç‰¹å¾å±‚çš„ç©ºé—´å°ºå¯¸
    input_spatial_shapes = torch.tensor([[50, 50], [25, 25], [13, 13], [7, 7]], dtype=torch.long).cuda()
    
    # è®¡ç®—æ¯å±‚çš„èµ·å§‹ç´¢å¼•å’Œæ€»é•¿åº¦
    level_sizes = (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).tolist()
    input_level_start_index = torch.tensor([0] + level_sizes[:-1], dtype=torch.long).cumsum(0).cuda()
    total_len = sum(level_sizes)
    
    # åˆ›å»ºè¾“å…¥å¼ é‡
    query = torch.randn(batch_size, num_queries, d_model).cuda()
    reference_points = torch.rand(batch_size, num_queries, n_levels, 2).cuda()
    input_flatten = torch.randn(batch_size, total_len, d_model).cuda()  # æ³¨æ„ï¼šé•¿åº¦æ˜¯ total_len
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = msda(
            query,
            reference_points,
            input_flatten,
            input_spatial_shapes,
            input_level_start_index,
            None
        )
    
    print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"      query å½¢çŠ¶: {query.shape}")
    print(f"      input_flatten å½¢çŠ¶: {input_flatten.shape}")
    print(f"      è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"      ç©ºé—´å°ºå¯¸: {input_spatial_shapes.tolist()}")
    print(f"      æ€»ä½ç½®æ•°: {total_len}")
    
except Exception as e:
    print(f"   âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "="*60)
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CUDA æ‰©å±•å·¥ä½œæ­£å¸¸")
print("="*60)
