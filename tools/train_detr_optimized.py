#!/usr/bin/env python3
"""
DETR è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ transformers åº“ + ä¼˜åŒ–çš„æ•°æ®åŠ è½½ï¼‰
- ä½¿ç”¨ torchvision.io.read_imageï¼ˆC++ è§£ç ï¼‰
- ä¼˜åŒ– DataLoader å‚æ•°
- æ”¯æŒ AMPã€checkpointã€è¯„ä¼°
"""

import argparse
import json
import signal
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from torchvision.io import read_image, ImageReadMode
import torchvision.transforms.functional as F
from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor

# DETR æ ‡å‡†å½’ä¸€åŒ–å‚æ•°ï¼ˆImageNetï¼‰
DETR_MEAN = [0.485, 0.456, 0.406]
DETR_STD = [0.229, 0.224, 0.225]


class CocoDetrDataset(torch.utils.data.Dataset):
    """COCO æ ¼å¼æ•°æ®é›†ï¼ˆC++ è§£ç  + transformers DETR æ ¼å¼ï¼‰"""
    
    def __init__(
        self,
        img_root: str,
        ann_file: str,
        min_size: int = 800,
        max_size: int = 1333,
        is_train: bool = True,
        blacklist_file: str = None,
    ):
        self.root = Path(img_root)
        self.coco = COCO(str(ann_file))
        self.ids = list(sorted(self.coco.imgs.keys()))
        
        # åŠ è½½é»‘åå•å¹¶è¿‡æ»¤
        if blacklist_file and Path(blacklist_file).exists():
            with open(blacklist_file) as f:
                blacklist_data = json.load(f)
            corrupted_paths = {item["path"] for item in blacklist_data.get("corrupted_images", [])}
            
            # è¿‡æ»¤æŸåå›¾åƒ
            original_count = len(self.ids)
            self.ids = [
                img_id for img_id in self.ids
                if str(self.root / self.coco.loadImgs(img_id)[0]["file_name"]) not in corrupted_paths
            ]
            filtered_count = original_count - len(self.ids)
            if filtered_count > 0:
                print(f"ğŸ“‹ é»‘åå•è¿‡æ»¤: {filtered_count} å¼ æŸåå›¾åƒå·²è·³è¿‡")
        
        # ç±»åˆ«æ˜ å°„åˆ°è¿ç»­ [0..N-1]
        cat_ids = sorted(self.coco.getCatIds())
        self.cat_id_map = {cat_id: i for i, cat_id in enumerate(cat_ids)}
        self.reverse_cat_id_map = {i: cat_id for i, cat_id in enumerate(cat_ids)}  # åå‘æ˜ å°„
        self.num_classes = len(cat_ids)
        
        self.min_size = min_size
        self.max_size = max_size
        self.is_train = is_train

    def __len__(self):
        return len(self.ids)

    def _resize(self, image: torch.Tensor, target: Dict) -> Tuple[torch.Tensor, Dict]:
        """ä¿æŒçºµæ¨ªæ¯”çš„ resize å¹¶è½¬æ¢ bbox ä¸ºå½’ä¸€åŒ– cxcywh"""
        c, h, w = image.shape
        
        min_original_size = float(min(h, w))
        max_original_size = float(max(h, w))
        
        if max_original_size / min_original_size * self.min_size > self.max_size:
            size = int(round(self.max_size * min_original_size / max_original_size))
        else:
            size = self.min_size
        
        scale_factor = size / min_original_size
        
        new_h = int(h * scale_factor)
        new_w = int(w * scale_factor)
        image = F.resize(image, [new_h, new_w])
        
        # è½¬æ¢ bboxï¼šxyxy åƒç´  -> å½’ä¸€åŒ– cxcywhï¼ˆDETR æ ‡å‡†æ ¼å¼ï¼‰
        if "boxes" in target and len(target["boxes"]) > 0:
            boxes = target["boxes"] * scale_factor  # å…ˆç¼©æ”¾åˆ° resize åçš„åƒç´ åæ ‡
            # xyxy -> cxcywh
            boxes_cxcywh = torch.zeros_like(boxes)
            boxes_cxcywh[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2  # cx
            boxes_cxcywh[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2  # cy
            boxes_cxcywh[:, 2] = boxes[:, 2] - boxes[:, 0]        # w
            boxes_cxcywh[:, 3] = boxes[:, 3] - boxes[:, 1]        # h
            # å½’ä¸€åŒ–
            boxes_cxcywh[:, [0, 2]] /= new_w
            boxes_cxcywh[:, [1, 3]] /= new_h
            # Clamp åˆ° [0, 1] é˜²æ­¢è¶Šç•Œæ ‡æ³¨å¯¼è‡´ loss å¼‚å¸¸
            boxes_cxcywh = torch.clamp(boxes_cxcywh, min=0.0, max=1.0)
            
            # è¿‡æ»¤é›¶å®½/é›¶é«˜æ¡†ï¼ˆclamp åå¯èƒ½å‡ºç°ï¼Œä¼šå¯¼è‡´ loss/åŒ¹é…ä¸ç¨³å®šï¼‰
            valid_mask = (boxes_cxcywh[:, 2] > 0) & (boxes_cxcywh[:, 3] > 0)
            boxes_cxcywh = boxes_cxcywh[valid_mask]
            target["class_labels"] = target["class_labels"][valid_mask]
            
            target["boxes"] = boxes_cxcywh
        
        # åˆ é™¤ area å­—æ®µï¼ˆclamp å area ä¼šä¸ä¸€è‡´ï¼Œä¸” DETR è®­ç»ƒä¸ä¾èµ– areaï¼‰
        if "area" in target:
            del target["area"]
        
        target["size"] = torch.tensor([new_h, new_w])  # resize åå°ºå¯¸
        target["orig_size"] = torch.tensor([h, w])    # åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        
        return image, target

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = self.root / img_info["file_name"]

        # C++ è§£ç ï¼ˆé»‘åå•è¿‡æ»¤ + åå¤‡è·³è¿‡ï¼‰
        try:
            img = read_image(str(img_path), mode=ImageReadMode.RGB).float() / 255.0
        except Exception as e:
            # æŸåå›¾åƒåå¤‡å¤„ç†ï¼ˆé»‘åå•ç¼ºå¤±/ä¸å®Œæ•´æ—¶ï¼‰
            print(f"\nâš ï¸  è·³è¿‡æŸåå›¾åƒ: {img_path} ({e})")
            print(f"   å»ºè®®è¿è¡Œ: python tools/scan_corrupted_images.py --ann {self.coco.dataset.get('info', {}).get('description', 'annotation')} --img-dir {self.root}\n")
            # è·³è¿‡åˆ°ä¸‹ä¸€ä¸ªï¼ˆé¿å…æ— é™é€’å½’ï¼‰
            if idx + 1 < len(self):
                return self.__getitem__(idx + 1)
            else:
                # æœ€åä¸€å¼ å›¾æŸåï¼Œè¿”å›ç¬¬ä¸€å¼ 
                return self.__getitem__(0)
        
        # DETR æ ‡å‡†å½’ä¸€åŒ–ï¼ˆImageNetï¼‰
        for c in range(3):
            img[c] = (img[c] - DETR_MEAN[c]) / DETR_STD[c]

        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)

        boxes = []
        labels = []
        areas = []

        for ann in anns:
            x, y, w, h = ann["bbox"]
            boxes.append([x, y, x + w, y + h])
            labels.append(self.cat_id_map[ann["category_id"]])
            areas.append(ann.get("area", w * h))

        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
            areas = torch.zeros((0,), dtype=torch.float32)
        else:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            labels = torch.tensor(labels, dtype=torch.int64)
            areas = torch.tensor(areas, dtype=torch.float32)

        target = {
            "boxes": boxes,
            "class_labels": labels,  # transformers ä½¿ç”¨ class_labels
            "image_id": torch.tensor([img_id]),
            "area": areas,
        }

        img, target = self._resize(img, target)
        return img, target


def collate_fn(batch):
    """è‡ªå®šä¹‰ collateï¼ˆtransformers DETR æ ¼å¼ï¼‰"""
    # è·å–batchä¸­çš„æœ€å¤§å°ºå¯¸
    max_h = max([img.shape[1] for img, _ in batch])
    max_w = max([img.shape[2] for img, _ in batch])
    
    # Padåˆ°ç›¸åŒå°ºå¯¸
    pixel_values = []
    pixel_mask = []
    labels = []
    
    for img, target in batch:
        c, h, w = img.shape
        padded_img = torch.zeros(c, max_h, max_w)
        padded_img[:, :h, :w] = img
        pixel_values.append(padded_img)
        
        # mask: 1è¡¨ç¤ºçœŸå®åƒç´ ï¼Œ0è¡¨ç¤ºpadding
        mask = torch.zeros(max_h, max_w, dtype=torch.long)
        mask[:h, :w] = 1
        pixel_mask.append(mask)
        
        labels.append(target)
    
    return {
        "pixel_values": torch.stack(pixel_values),
        "pixel_mask": torch.stack(pixel_mask),
        "labels": labels,
    }


def build_model(num_classes: int, pretrained: bool = True, offline_mode: bool = False):
    """æ„å»º DETR æ¨¡å‹"""
    if pretrained:
        try:
            # ä¼˜å…ˆå°è¯•æœ¬åœ°ç¼“å­˜
            model = DetrForObjectDetection.from_pretrained(
                "facebook/detr-resnet-50",
                num_labels=num_classes,
                ignore_mismatched_sizes=True,
                local_files_only=offline_mode,
            )
        except Exception as e:
            if offline_mode:
                print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æ— æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {e}")
                print("âš ï¸  å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
                config = DetrConfig(num_labels=num_classes, num_queries=100)
                model = DetrForObjectDetection(config)
            else:
                # éç¦»çº¿æ¨¡å¼ï¼Œå…è®¸ä¸‹è½½
                model = DetrForObjectDetection.from_pretrained(
                    "facebook/detr-resnet-50",
                    num_labels=num_classes,
                    ignore_mismatched_sizes=True,
                )
    else:
        config = DetrConfig(num_labels=num_classes, num_queries=100)
        model = DetrForObjectDetection(config)
    
    return model


def save_checkpoint(model, optimizer, epoch, iteration, best_map, output_dir, is_best=False):
    """ä¿å­˜ checkpoint"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "epoch": epoch,
        "iteration": iteration,
        "best_map": best_map,
    }
    
    torch.save(checkpoint, output_dir / "last.pth")
    
    if is_best:
        torch.save(checkpoint, output_dir / "best.pth")
        print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: mAP={best_map:.4f}")


def load_checkpoint(checkpoint_path, model, optimizer=None):
    """åŠ è½½ checkpoint"""
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model"])
    
    if optimizer and "optimizer" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer"])
    
    return checkpoint.get("epoch", 0), checkpoint.get("iteration", 0), checkpoint.get("best_map", 0.0)


@torch.no_grad()
def evaluate(model, data_loader, device, coco_gt, reverse_cat_id_map=None, processor=None, score_threshold=0.05, offline_mode=False):
    """COCO è¯„ä¼°ï¼ˆä½¿ç”¨ DetrImageProcessor.post_process_object_detectionï¼‰"""
    model.eval()
    
    try:
        # åˆ›å»º processorï¼ˆç”¨äº post_processï¼‰
        if processor is None:
            try:
                processor = DetrImageProcessor.from_pretrained(
                    "facebook/detr-resnet-50",
                    local_files_only=True  # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
                )
            except Exception as e:
                if offline_mode:
                    print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æ— æ³•åŠ è½½ DetrImageProcessor ç¼“å­˜ï¼Œè·³è¿‡è¯„ä¼°: {e}")
                    return None  # è·³è¿‡è¯„ä¼°
                # ç¼“å­˜ä¸å­˜åœ¨æ—¶æ‰ä¸‹è½½
                processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
        
        results = []
        
        for batch in data_loader:
            pixel_values = batch["pixel_values"].to(device, non_blocking=True)
            pixel_mask = batch["pixel_mask"].to(device, non_blocking=True)
            labels = batch["labels"]
            
            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)
            
            # ä½¿ç”¨å®˜æ–¹ post_processï¼ˆæ­£ç¡®å¤„ç† padding å’Œå½’ä¸€åŒ–ï¼‰
            # target_sizes: åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆæœª resizeï¼Œç”¨äºå°†é¢„æµ‹æ¡†åæ ‡è½¬æ¢å›åŸå§‹åæ ‡ç³»ï¼‰
            target_sizes = torch.stack([l["orig_size"] for l in labels]).to(device)
            
            # post_process_object_detection è¿”å› [{'scores', 'labels', 'boxes'}, ...]
            results_per_image = processor.post_process_object_detection(
                outputs, 
                threshold=score_threshold,  # ä½¿ç”¨å¯è°ƒç½®ä¿¡åº¦é˜ˆå€¼
                target_sizes=target_sizes
            )
            
            for i, result in enumerate(results_per_image):
                image_id = labels[i]["image_id"].item()
                
                boxes = result["boxes"].cpu()  # xyxy åƒç´ åæ ‡ï¼ˆåŸå§‹å›¾åƒåæ ‡ç³»ï¼‰
                scores = result["scores"].cpu()
                pred_labels = result["labels"].cpu()
                
                for box, score, label in zip(boxes, scores, pred_labels):
                    x1, y1, x2, y2 = box.tolist()
                    
                    # åå‘æ˜ å°„åˆ°åŸå§‹ category_idï¼ˆå¦‚æœæä¾›ï¼‰
                    original_cat_id = label.item()
                    if reverse_cat_id_map is not None:
                        original_cat_id = reverse_cat_id_map.get(label.item(), label.item())
                    
                    results.append({
                        "image_id": image_id,
                        "category_id": original_cat_id,  # ä½¿ç”¨åŸå§‹ category_id
                        "bbox": [x1, y1, x2 - x1, y2 - y1],  # COCO æ ¼å¼ xywh
                        "score": score.item(),
                    })
        
        if not results:
            return {"mAP": 0.0, "AP_small": 0.0}
        
        coco_dt = coco_gt.loadRes(results)
        coco_eval = COCOeval(coco_gt, coco_dt, iouType="bbox")
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        
        return {
            "mAP": coco_eval.stats[0],
            "AP50": coco_eval.stats[1],
            "AP75": coco_eval.stats[2],
            "AP_small": coco_eval.stats[3],
            "AP_medium": coco_eval.stats[4],
            "AP_large": coco_eval.stats[5],
        }
    finally:
        # ç¡®ä¿æ‰€æœ‰è¿”å›è·¯å¾„éƒ½æ¢å¤è®­ç»ƒæ¨¡å¼ï¼ˆåŒ…æ‹¬å¼‚å¸¸/æå‰è¿”å›ï¼‰
        model.train()


def train_one_epoch(model, optimizer, data_loader, device, epoch, use_amp, amp_dtype, 
                    scaler=None, print_freq=50, grad_accum=1, clip_max_norm=None):
    """è®­ç»ƒä¸€ä¸ª epoch
    
    Args:
        grad_accum: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆbatch = batch_size * grad_accumï¼‰
        clip_max_norm: æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°ï¼ˆNone=ä¸è£å‰ªï¼‰
    """
    model.train()
    total_loss = 0.0
    start_time = time.time()
    
    # åˆ†æ®µè®¡æ—¶ç»Ÿè®¡
    total_t_load = 0.0
    total_t_step = 0.0
    t_batch_start = time.time()
    
    for step, batch in enumerate(data_loader, start=1):
        # æ•°æ®åŠ è½½è€—æ—¶
        t_load = time.time() - t_batch_start
        total_t_load += t_load
        
        t_step_start = time.time()
        pixel_values = batch["pixel_values"].to(device, non_blocking=True)
        pixel_mask = batch["pixel_mask"].to(device, non_blocking=True)
        labels = batch["labels"]
        
        # è½¬æ¢ labels æ ¼å¼ï¼ˆboxes å·²ç»æ˜¯å½’ä¸€åŒ– cxcywhï¼‰
        batch_labels = []
        for target in labels:
            batch_labels.append({
                "class_labels": target["class_labels"].to(device, non_blocking=True),
                "boxes": target["boxes"].to(device, non_blocking=True),  # å½’ä¸€åŒ– cxcywh
            })
        
        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):
            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=batch_labels)
            loss = outputs.loss
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šlossç¼©æ”¾ï¼Œé˜²æ­¢ç´¯ç§¯åæ¢¯åº¦æ”¾å¤§
            if grad_accum > 1:
                loss = loss / grad_accum
        
        if scaler is not None:
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯grad_accumæ­¥æ›´æ–°ä¸€æ¬¡
            if step % grad_accum == 0:
                # æ¢¯åº¦è£å‰ªï¼ˆåœ¨unscaleåã€stepå‰ï¼‰
                if clip_max_norm is not None:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)
                
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
        else:
            loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯grad_accumæ­¥æ›´æ–°ä¸€æ¬¡
            if step % grad_accum == 0:
                # æ¢¯åº¦è£å‰ª
                if clip_max_norm is not None:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)
                
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
        
        # è®°å½•åŸå§‹lossï¼ˆæœªç¼©æ”¾ï¼‰
        total_loss += loss.item() * (grad_accum if grad_accum > 1 else 1)
        
        # è®­ç»ƒè®¡ç®—è€—æ—¶
        torch.cuda.synchronize() if device.type == "cuda" else None
        t_step = time.time() - t_step_start
        total_t_step += t_step
        
        if step % print_freq == 0:
            elapsed = time.time() - start_time
            it_s = step / elapsed
            avg_loss = total_loss / step
            
            # è®¡ç®—è€—æ—¶å æ¯”
            avg_t_load = total_t_load / step
            avg_t_step = total_t_step / step
            pct_load = 100.0 * total_t_load / elapsed
            pct_step = 100.0 * total_t_step / elapsed
            
            # æ˜¾ç¤ºå®é™…losså€¼ï¼ˆå·²è¿˜åŸï¼‰
            actual_loss = loss.item() * (grad_accum if grad_accum > 1 else 1)
            print(f"Epoch [{epoch}] Step [{step}/{len(data_loader)}] "
                  f"Loss: {actual_loss:.4f} (avg: {avg_loss:.4f}) | Speed: {it_s:.2f} it/s")
            print(f"  â±ï¸  t_load: {avg_t_load:.3f}s ({pct_load:.1f}%) | t_step: {avg_t_step:.3f}s ({pct_step:.1f}%)")
        
        t_batch_start = time.time()
    
    # åˆ·æ–°æ®‹ç•™æ¢¯åº¦ï¼ˆå½“batchæ•°ä¸èƒ½è¢«grad_accumæ•´é™¤æ—¶ï¼‰
    if grad_accum > 1 and len(data_loader) % grad_accum != 0:
        if scaler is not None:
            if clip_max_norm is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            if clip_max_norm is not None:
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)
            optimizer.step()
        optimizer.zero_grad(set_to_none=True)
        print(f"  ğŸ”„ åˆ·æ–°æ®‹ç•™æ¢¯åº¦ï¼ˆ{len(data_loader)} % {grad_accum} = {len(data_loader) % grad_accum} ä¸ª batchï¼‰")
    
    return {"loss": total_loss / len(data_loader)}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-img", required=True)
    parser.add_argument("--train-ann", required=True)
    parser.add_argument("--val-img")
    parser.add_argument("--val-ann")
    parser.add_argument("--num-classes", type=int, default=3)
    parser.add_argument("--pretrained", action="store_true")
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--num-epochs", type=int, default=50)
    parser.add_argument("--lr", type=float, default=5e-5)
    parser.add_argument("--weight-decay", type=float, default=1e-4)
    parser.add_argument("--num-workers", type=int, default=12)
    parser.add_argument("--prefetch-factor", type=int, default=2)
    parser.add_argument("--min-size", type=int, default=800)
    parser.add_argument("--max-size", type=int, default=1333)
    parser.add_argument("--amp", action="store_true")
    parser.add_argument("--device", default="cuda")
    parser.add_argument("--subset", type=int)
    parser.add_argument("--output-dir", default="outputs/detr_optimized")
    parser.add_argument("--resume")
    parser.add_argument("--eval-interval", type=int, default=5)
    parser.add_argument("--print-freq", type=int, default=50)
    parser.add_argument("--score-threshold", type=float, default=0.05, help="è¯„ä¼°æ—¶çš„ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--offline", action="store_true", help="ç¦»çº¿æ¨¡å¼ï¼Œä¸ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--no-eval", action="store_true", help="è·³è¿‡è¯„ä¼°ï¼ˆç¦»çº¿æ— ç¼“å­˜æ—¶è‡ªåŠ¨è·³è¿‡ï¼‰")
    parser.add_argument("--blacklist", help="æŸåå›¾åƒé»‘åå•æ–‡ä»¶")
    parser.add_argument("--grad-accum", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆbatch=batch_size*grad_accumï¼‰")
    parser.add_argument("--clip-max-norm", type=float, help="æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°ï¼ˆæ¨è0.1ï¼ŒNone=ä¸è£å‰ªï¼‰")
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    device = torch.device(args.device)
    
    # å…¨å±€å˜é‡ç”¨äºä¿¡å·å¤„ç†
    interrupted = False
    checkpoint_data = {}
    
    def save_interrupt_checkpoint(model, optimizer, scheduler, epoch, output_dir):
        """ä¿å­˜ä¸­æ–­æ—¶çš„checkpoint"""
        ckpt_path = output_dir / "interrupted.pth"
        torch.save({
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        }, ckpt_path)
        print(f"\nğŸ’¾ å·²ä¿å­˜ä¸­æ–­checkpoint: {ckpt_path}")
    
    def signal_handler(signum, frame):
        """å¤„ç†Ctrl+Cä¿¡å·"""
        nonlocal interrupted
        print(f"\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)ï¼Œæ­£åœ¨ä¿å­˜checkpoint...")
        interrupted = True
        if checkpoint_data:
            save_interrupt_checkpoint(**checkpoint_data)
        print("âœ… Checkpointå·²ä¿å­˜ï¼Œè®­ç»ƒå°†åœ¨å½“å‰epochç»“æŸååœæ­¢")
        print("   å¯ä½¿ç”¨ --resume interrupted.pth æ¢å¤è®­ç»ƒ\n")
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    
    torch.backends.cudnn.benchmark = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision("high")
    
    print("=" * 80)
    print("ğŸš€ DETR è®­ç»ƒï¼ˆtransformers + ä¼˜åŒ–æ•°æ®åŠ è½½ï¼‰")
    print("=" * 80)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è®¾å¤‡: {device}")
    print(f"Batch Size: {args.batch_size} | Workers: {args.num_workers}")
    print(f"å›¾åƒå°ºå¯¸: min={args.min_size}, max={args.max_size}")
    
    # æ¢¯åº¦ç´¯ç§¯å’Œè£å‰ªä¿¡æ¯
    if args.grad_accum > 1:
        effective_batch = args.batch_size * args.grad_accum
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.grad_accum} æ­¥ | æœ‰æ•ˆBatch: {effective_batch}")
    if args.clip_max_norm is not None:
        print(f"æ¢¯åº¦è£å‰ª: clip_max_norm={args.clip_max_norm}")
    
    print("=" * 80)
    
    # æ•°æ®é›†
    train_dataset = CocoDetrDataset(
        args.train_img, args.train_ann, args.min_size, args.max_size, 
        blacklist_file=args.blacklist
    )
    
    # è‡ªåŠ¨æ¨æ–­ç±»åˆ«æ•°ï¼ˆä»æ•°æ®é›†ï¼‰
    actual_num_classes = train_dataset.num_classes
    if args.num_classes != actual_num_classes:
        print(f"âš ï¸  å‘½ä»¤è¡ŒæŒ‡å®š --num-classes={args.num_classes}ï¼Œä½†æ•°æ®é›†æœ‰ {actual_num_classes} ä¸ªç±»åˆ«")
        print(f"    è‡ªåŠ¨ä½¿ç”¨æ•°æ®é›†ç±»åˆ«æ•°: {actual_num_classes}")
        args.num_classes = actual_num_classes
    
    # å†™å…¥é…ç½®ï¼ˆåœ¨ num_classes æ ¡éªŒåï¼Œç¡®ä¿é…ç½®è®°å½•å‡†ç¡®ï¼‰
    with open(output_dir / "config.json", "w") as f:
        json.dump(vars(args), f, indent=2)
    
    if args.subset:
        train_dataset = Subset(train_dataset, range(min(args.subset, len(train_dataset))))
        print(f"ğŸ“Š ä½¿ç”¨è®­ç»ƒå­é›†: {len(train_dataset)} å¼ å›¾åƒ")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        persistent_workers=args.num_workers > 0,
        prefetch_factor=args.prefetch_factor if args.num_workers > 0 else None,
    )
    
    val_loader = None
    coco_gt = None
    temp_ann_file = None  # è®°å½•ä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºæ¸…ç†
    if args.val_img and args.val_ann:
        val_dataset_base = CocoDetrDataset(args.val_img, args.val_ann, args.min_size, args.max_size, is_train=False)
        val_dataset = val_dataset_base
        
        if args.subset:
            subset_size = min(args.subset // 4, len(val_dataset_base))
            
            # é˜²æ­¢ subset éªŒè¯é›†ä¸ºç©ºï¼ˆargs.subset < 4 æ—¶ï¼‰
            if subset_size == 0:
                print("âš ï¸  subset å¤ªå°ï¼ŒéªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°")
                val_loader = None
            else:
                val_dataset = Subset(val_dataset_base, range(subset_size))
                
                # åˆ›å»ºåªåŒ…å« subset å›¾åƒçš„ä¸´æ—¶ COCOï¼ˆç”¨äºå‡†ç¡®è¯„ä¼°ï¼‰
                subset_img_ids = [val_dataset_base.ids[i] for i in range(subset_size)]
                coco_full = COCO(args.val_ann)
                
                # æ„å»ºå­é›†æ ‡æ³¨
                subset_anns = {
                    "images": [img for img in coco_full.dataset["images"] if img["id"] in subset_img_ids],
                    "annotations": [ann for ann in coco_full.dataset["annotations"] if ann["image_id"] in subset_img_ids],
                    "categories": coco_full.dataset["categories"]
                }
                
                # åˆ›å»ºä¸´æ—¶ COCO å¯¹è±¡
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json.dump(subset_anns, f)
                    temp_ann_file = f.name  # ä¿å­˜è·¯å¾„ç”¨äºæ¸…ç†
                
                coco_gt = COCO(temp_ann_file)
                print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒï¼ˆsubset æ¨¡å¼ï¼Œä½¿ç”¨å­é›†æ ‡æ³¨ï¼‰")
        else:
            coco_gt = COCO(args.val_ann)
            print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ")
        
        if val_loader is None and args.subset and subset_size == 0:
            pass  # å·²è·³è¿‡
        else:
            val_loader = DataLoader(
                val_dataset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                collate_fn=collate_fn,
                pin_memory=True,
                prefetch_factor=args.prefetch_factor if args.num_workers > 0 else None,
            )
    
    # æ¨¡å‹
    print(f"ğŸ“¦ æ„å»ºæ¨¡å‹: num_classes={args.num_classes}")
    model = build_model(args.num_classes, args.pretrained, args.offline)
    model.to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    amp_dtype = torch.bfloat16 if args.amp and torch.cuda.is_bf16_supported() else torch.float16
    use_amp = args.amp and device.type == "cuda"
    
    # åˆå§‹åŒ– GradScalerï¼ˆfp16 æ—¶éœ€è¦ï¼‰
    scaler = None
    if use_amp and amp_dtype == torch.float16:
        scaler = torch.cuda.amp.GradScaler()
        print(f"âš¡ ä½¿ç”¨ AMP (fp16) + GradScaler")
    elif use_amp:
        print(f"âš¡ ä½¿ç”¨ AMP (bf16)")
    
    start_epoch = 0
    best_map = 0.0
    
    if args.resume:
        start_epoch, _, best_map = load_checkpoint(args.resume, model, optimizer)
    
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    
    metrics_log = []
    
    for epoch in range(start_epoch, args.num_epochs):
        # æ›´æ–°checkpointæ•°æ®ä¾›ä¿¡å·å¤„ç†å™¨ä½¿ç”¨
        checkpoint_data.update({
            'model': model,
            'optimizer': optimizer,
            'scheduler': None,  # å¦‚æœæœ‰schedulerå¯ä»¥åœ¨è¿™é‡Œæ›´æ–°
            'epoch': epoch,
            'output_dir': output_dir
        })
        
        # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
        if interrupted:
            print("\nğŸ›‘ è®­ç»ƒå·²è¢«ç”¨æˆ·ä¸­æ–­")
            break
        
        epoch_start = time.time()
        
        train_metrics = train_one_epoch(
            model, optimizer, train_loader, device, epoch + 1,
            use_amp, amp_dtype, scaler, args.print_freq,
            grad_accum=args.grad_accum,
            clip_max_norm=args.clip_max_norm
        )
        
        epoch_time = time.time() - epoch_start
        
        eval_metrics = {}
        is_best = False
        
        if val_loader and ((epoch + 1) % args.eval_interval == 0 or epoch == args.num_epochs - 1) and not args.no_eval:
            print(f"\nğŸ“Š è¯„ä¼° Epoch {epoch + 1}...")
            # ä¼ é€’åå‘æ˜ å°„å­—å…¸
            reverse_map = getattr(val_loader.dataset.dataset if hasattr(val_loader.dataset, 'dataset') else val_loader.dataset, 'reverse_cat_id_map', None)
            eval_metrics = evaluate(
                model, val_loader, device, coco_gt, 
                reverse_cat_id_map=reverse_map,
                score_threshold=args.score_threshold,
                offline_mode=args.offline
            )
            
            # ç¦»çº¿æ¨¡å¼æ— ç¼“å­˜æ—¶ evaluate è¿”å› None
            if eval_metrics is None:
                print("âš ï¸  è¯„ä¼°å·²è·³è¿‡ï¼ˆç¦»çº¿æ¨¡å¼æ—  processor ç¼“å­˜ï¼‰")
                eval_metrics = {}
            elif eval_metrics.get("mAP", 0) > best_map:
                best_map = eval_metrics["mAP"]
                is_best = True
        
        save_checkpoint(model, optimizer, epoch + 1, 0, best_map, output_dir, is_best)
        
        log_entry = {
            "epoch": epoch + 1,
            "train_loss": train_metrics["loss"],
            "epoch_time": epoch_time,
            **eval_metrics,
        }
        metrics_log.append(log_entry)
        
        with open(output_dir / "metrics.json", "w") as f:
            json.dump(metrics_log, f, indent=2)
        
        print(f"\n{'=' * 80}")
        print(f"Epoch [{epoch + 1}/{args.num_epochs}] å®Œæˆ | è€—æ—¶: {epoch_time:.1f}s")
        print(f"è®­ç»ƒ Loss: {train_metrics['loss']:.4f}")
        if eval_metrics:
            print(f"éªŒè¯ mAP: {eval_metrics['mAP']:.4f} | AP_small: {eval_metrics['AP_small']:.4f}")
            print(f"æœ€ä½³ mAP: {best_map:.4f}")
        print(f"{'=' * 80}\n")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if temp_ann_file and Path(temp_ann_file).exists():
        Path(temp_ann_file).unlink()
        print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ ‡æ³¨æ–‡ä»¶: {temp_ann_file}")
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
