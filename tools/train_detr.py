#!/usr/bin/env python3
"""
DETR è®­ç»ƒè„šæœ¬
æ”¯æŒä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼Œè¿›è¡Œäº¤é€šåœºæ™¯ç›®æ ‡æ£€æµ‹è®­ç»ƒ
"""

import argparse
import os
import sys
import time
from pathlib import Path

import torch
import yaml
from tqdm import tqdm
from pycocotools.coco import COCO
from transformers import DetrImageProcessor
from torch.amp import autocast, GradScaler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dataset import build_dataloader
from models import build_detr_model
from utils import MetricsLogger, save_checkpoint, load_checkpoint, setup_logger
from tools.eval_detr import evaluate


def load_config(config_path: str) -> dict:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def setup_output_dir(config: dict, args) -> Path:
    """è®¾ç½®è¾“å‡ºç›®å½•"""
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        base_dir = config['output']['base_dir']
        exp_name = config['output']['experiment_name']
        output_dir = Path(base_dir) / exp_name
    
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def save_config(config: dict, output_dir: Path):
    """ä¿å­˜é…ç½®åˆ°è¾“å‡ºç›®å½•"""
    config_save_path = output_dir / "config.yaml"
    with open(config_save_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False)
    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_save_path}")


def train_one_epoch(
    model,
    dataloader,
    optimizer,
    device,
    epoch,
    image_processor,
    max_iters,
    log_interval,
    logger,
    scaler=None,
    use_amp=False,
    amp_dtype=None,
):
    """è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        scaler: AMP GradScalerï¼ˆå¦‚å¯ç”¨AMPï¼‰
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        amp_dtype: AMPæ•°æ®ç±»å‹ï¼ˆtorch.float16æˆ–torch.bfloat16ï¼‰
    """
    model.train()
    
    epoch_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for batch_idx, encoding in enumerate(pbar):
        # encodingå·²ç»æ˜¯é¢„å¤„ç†å¥½çš„dictï¼ŒåŒ…å«pixel_values, pixel_mask, labels
        # ç”±collate_fnåœ¨workerè¿›ç¨‹ä¸­å¹¶è¡Œå¤„ç†å®Œæˆ
        
        # ç§»åˆ°è®¾å¤‡
        pixel_values = encoding['pixel_values'].to(device)
        pixel_mask = encoding['pixel_mask'].to(device)
        labels = encoding['labels']  # å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼
        
        # å°†labelsç§»åˆ°è®¾å¤‡
        labels = [
            {
                'class_labels': item['class_labels'].to(device),
                'boxes': item['boxes'].to(device),
            }
            for item in labels
        ]
        
        # å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒAMPï¼‰
        if use_amp:
            with autocast('cuda', dtype=amp_dtype):
                outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)
                loss = outputs.loss
        else:
            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)
            loss = outputs.loss
        
        # åå‘ä¼ æ’­ï¼ˆæ”¯æŒAMPï¼‰
        optimizer.zero_grad()
        if use_amp and scaler is not None:
            scaler.scale(loss).backward()
            # æ¢¯åº¦è£å‰ªï¼ˆå¯ç”¨ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            # æ¢¯åº¦è£å‰ªï¼ˆå¯ç”¨ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
            optimizer.step()
        
        # è®°å½•
        epoch_loss += loss.item()
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'avg_loss': f"{epoch_loss / num_batches:.4f}"
        })
        
        # æ—¥å¿—è¾“å‡º
        if (batch_idx + 1) % log_interval == 0:
            logger.info(
                f"Epoch [{epoch}] Iter [{batch_idx + 1}/{len(dataloader)}] "
                f"Loss: {loss.item():.4f} Avg Loss: {epoch_loss / num_batches:.4f}"
            )
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£
        if max_iters and num_batches >= max_iters:
            logger.info(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ•° {max_iters}ï¼Œåœæ­¢è®­ç»ƒ")
            break
    
    return epoch_loss / num_batches if num_batches > 0 else 0.0


def train(config: dict, args):
    """
    è®­ç»ƒä¸»å‡½æ•°
    """
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ DETR æ¨¡å‹")
    print("="*60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config['device']['type'] if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ”§ è®¾å¤‡: {device}")
    
    # ===== é«˜çº§ä¼˜åŒ–é…ç½® =====
    
    # 1. TF32 åŠ é€Ÿï¼ˆAmpereæ¶æ„å…è´¹æé€Ÿï¼Œå‡ ä¹æ— ç²¾åº¦æŸå¤±ï¼‰
    if torch.cuda.is_available() and hasattr(torch.backends.cuda, 'matmul'):
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print("âœ… TF32 å·²å¯ç”¨ï¼ˆAmpereæ¶æ„åŠ é€Ÿï¼‰")
    
    # 2. AMP é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨BF16ï¼Œå›é€€FP16ï¼‰
    use_amp = config['training'].get('amp', False) and torch.cuda.is_available()
    amp_dtype = None
    if use_amp:
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒBF16ï¼ˆAmpereåŠä»¥ä¸Šæ¶æ„ï¼‰
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            amp_dtype = torch.bfloat16
            print("âœ… AMPä½¿ç”¨BF16ï¼ˆæ›´ç¨³å®šï¼ŒåŠ¨æ€èŒƒå›´æ›´å¤§ï¼‰")
        else:
            amp_dtype = torch.float16
            print("âš ï¸  AMPä½¿ç”¨FP16ï¼ˆBF16ä¸æ”¯æŒï¼Œä½¿ç”¨ä¼ ç»Ÿæ··åˆç²¾åº¦ï¼‰")
    
    scaler = GradScaler('cuda') if (use_amp and amp_dtype == torch.float16) else None
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  æ•°æ®é›†: {config['dataset']['name']}")
    print(f"  ç±»åˆ«æ•°: {config['dataset']['num_classes']}")
    print(f"  æ¨¡å‹: {config['model']['name']}")
    print(f"  Batch Size: {config['training']['batch_size']}")
    print(f"  Max Epochs: {config['training']['max_epochs']}")
    print(f"  å­¦ä¹ ç‡: {config['training']['optimizer']['lr']}")
    print(f"  AMP æ··åˆç²¾åº¦: {'âœ“ å¯ç”¨' if use_amp else 'âœ— ç¦ç”¨'}")
    
    if args.max_iter:
        print(f"  æœ€å¤§è¿­ä»£: {args.max_iter}")
        config['training']['max_iters'] = args.max_iter
    
    # å­é›†é‡‡æ ·é…ç½®
    subset_size = args.subset_size or config['training'].get('subset_size')
    if subset_size:
        print(f"  å­é›†å¤§å°: {subset_size}")
        config['training']['subset_size'] = subset_size
    
    overfit_mode = args.overfit or config['training'].get('overfit', False)
    if overfit_mode:
        print(f"  âš ï¸  è¿‡æ‹Ÿåˆæ¨¡å¼ï¼šå·²å¯ç”¨ï¼ˆç”¨äºéªŒè¯è®­ç»ƒæµç¨‹ï¼‰")
        config['training']['overfit'] = True
        
        # è®¾ç½®å…¨å±€éšæœºç§å­ï¼ˆä¿è¯è¿‡æ‹Ÿåˆæµ‹è¯•å¯å¤ç°ï¼‰
        import random
        import numpy as np
        seed = config['training'].get('subset_seed', 42)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        print(f"  ğŸ² å…¨å±€éšæœºç§å­å·²è®¾ç½®: {seed}ï¼ˆä¿è¯å¯å¤ç°ï¼‰")
    
    # Progressive Resizing é…ç½®
    resize_schedule = config['training'].get('resize_schedule')
    if resize_schedule:
        print(f"  Progressive Resizing: {resize_schedule}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_output_dir(config, args)
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜é…ç½®
    save_config(config, output_dir)
    
    # Resume æ£€æŸ¥ï¼ˆåœ¨ä½¿ç”¨å‰å®šä¹‰ï¼‰
    resume_checkpoint = args.resume or config['training'].get('resume')
    is_resume = bool(resume_checkpoint)
    
    # è®¾ç½®æ—¥å¿—ï¼ˆResume æ¨¡å¼ç»­å†™ï¼‰
    logger = setup_logger('train', output_dir / 'train.log')
    metrics_logger = MetricsLogger(output_dir, resume=is_resume)
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("\n" + "="*60)
    print("ğŸ“¦ æ„å»ºæ•°æ®åŠ è½½å™¨")
    print("="*60)
    
    # å…ˆåˆ›å»ºimage_processorï¼ˆç”¨äºåœ¨workerä¸­é¢„å¤„ç†ï¼‰
    model_name = config['model']['name']
    if not model_name.startswith('facebook/'):
        model_name = f'facebook/{model_name}'
    image_processor = DetrImageProcessor.from_pretrained(model_name)
    
    # æ„å»ºDataLoaderï¼ˆä¼ å…¥processorå®ç°workerä¸­å¹¶è¡Œé¢„å¤„ç†ï¼‰
    train_loader = build_dataloader(config, 'train', image_processor=image_processor)
    val_loader = build_dataloader(config, 'val', image_processor=image_processor)
    
    # æ„å»ºæ¨¡å‹
    print("\n" + "="*60)
    print("ğŸ—ï¸  æ„å»ºæ¨¡å‹")
    print("="*60)
    
    model = build_detr_model(config)
    model = model.to(device)
    
    # ===== torch.compile ä¼˜åŒ–ï¼ˆPyTorch 2.0+ TransformeråŠ é€Ÿï¼‰=====
    use_compile = config['training'].get('compile', False)
    if use_compile and hasattr(torch, 'compile'):
        print("\nğŸš€ å¯ç”¨ torch.compile ä¼˜åŒ–...")
        try:
            # mode='reduce-overhead' å¯¹Transformeræ•ˆæœå¥½
            model = torch.compile(model, mode='reduce-overhead')
            print("âœ… torch.compile å¯ç”¨æˆåŠŸï¼ˆé¢„æœŸæé€Ÿ10-30%ï¼‰")
        except Exception as e:
            print(f"âš ï¸  torch.compile å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨eageræ¨¡å¼: {e}")
    
    # æ„å»ºä¼˜åŒ–å™¨
    print("\nğŸ“Š æ„å»ºä¼˜åŒ–å™¨")
    optimizer_config = config['training']['optimizer']
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=float(optimizer_config['lr']),
        weight_decay=float(optimizer_config['weight_decay']),
        betas=tuple(optimizer_config['betas']),
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=int(config['training']['lr_scheduler']['step_size']),
        gamma=float(config['training']['lr_scheduler']['gamma']),
    )
    
    # Resume é€»è¾‘ï¼šä» checkpoint æ¢å¤ï¼ˆå·²åœ¨å‰é¢å®šä¹‰ resume_checkpointï¼‰
    start_epoch = 1
    best_metric_value = None
    loaded_checkpoint = None  # ç”¨äºåç»­è®¿é—®checkpointå­—å…¸
    
    if resume_checkpoint:
        print("\n" + "="*60)
        print(f"ğŸ”„ ä» checkpoint æ¢å¤è®­ç»ƒ: {resume_checkpoint}")
        print("="*60)
        loaded_checkpoint = load_checkpoint(
            checkpoint_path=Path(resume_checkpoint),
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            scaler=scaler,
            device=device,
            restore_rng_state=True,
        )
        start_epoch = loaded_checkpoint.get('epoch', 0) + 1  # ä»ä¸‹ä¸€ä¸ªepochç»§ç»­
        best_metric_value = loaded_checkpoint.get('best_metric')
        print(f"å°†ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
        if best_metric_value is not None:
            print(f"å†å²æœ€ä½³æŒ‡æ ‡: {best_metric_value:.4f}")
    
    # è®­ç»ƒå¾ªç¯
    print("\n" + "="*60)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("="*60)
    
    max_epochs = config['training']['max_epochs']
    max_iters = config['training'].get('max_iters')
    log_interval = config['training']['log_interval']
    save_interval = config['training']['save_interval']
    eval_interval = config['training'].get('eval_interval', 5)  # é»˜è®¤æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
    
    # åŠ è½½COCO ground truthç”¨äºéªŒè¯
    root_dir = Path(config['dataset']['root_dir'])
    val_ann_file = root_dir / config['dataset']['val_ann']
    coco_gt = COCO(val_ann_file)
    
    # Resumeæ—¶æ¢å¤best_lossï¼ˆé¿å…validationè·³è¿‡æ—¶ç¬¬ä¸€ä¸ªepochæ€»æ˜¯è¦†ç›–best.pthï¼‰
    best_loss = loaded_checkpoint.get('best_loss', float('inf')) if loaded_checkpoint else float('inf')
    best_map = 0.0 if best_metric_value is None else best_metric_value
    start_time = time.time()
    
    # æ£€æŸ¥ Resume å epoch èŒƒå›´æ˜¯å¦æœ‰æ•ˆ
    skip_training = False
    if start_epoch > max_epochs:
        logger.warning(f"âš ï¸  Resume çš„èµ·å§‹ epoch ({start_epoch}) å·²è¶…è¿‡ max_epochs ({max_epochs})")
        logger.warning(f"    â†’ è®­ç»ƒå°†ç›´æ¥ç»“æŸï¼Œä¸ä¼šæ‰§è¡Œæ–°çš„ epoch")
        logger.warning(f"    â†’ å»ºè®®å¢åŠ  max_epochs æˆ–æ£€æŸ¥ checkpoint")
        logger.warning(f"    â†’ å°†è·³è¿‡è®­ç»ƒå’Œæœ€ç»ˆä¿å­˜ï¼Œé¿å…è¦†ç›–å·²æœ‰ checkpoint")
        skip_training = True
    
    for epoch in range(start_epoch, max_epochs + 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"Epoch {epoch}/{max_epochs}")
        logger.info(f"{'='*60}")
        
        # Progressive Resizing: æ ¹æ® epoch è°ƒæ•´è¾“å…¥åˆ†è¾¨ç‡
        if resize_schedule:
            # resize_schedule æ ¼å¼: [[epoch1, size1], [epoch2, size2], ...]
            # æˆ– [[epoch1, {"shortest": s, "longest": l}], ...]
            current_size = None
            for schedule_epoch, size_config in resize_schedule:
                if epoch >= schedule_epoch:
                    current_size = size_config
            
            if current_size:
                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šæ•´æ•°æˆ–å­—å…¸
                if isinstance(current_size, dict):
                    # å­—å…¸æ ¼å¼ï¼šå…¼å®¹ä¸¤ç§é”®å
                    # {"shortest": 640, "longest": 1333} æˆ– {"shortest_edge": 640, "longest_edge": 1333}
                    shortest = current_size.get('shortest') or current_size.get('shortest_edge', 800)
                    longest = current_size.get('longest') or current_size.get('longest_edge', 1333)
                else:
                    # æ•´æ•°æ ¼å¼ï¼šçŸ­è¾¹ä¸ºè¯¥å€¼ï¼Œé•¿è¾¹ä½¿ç”¨é»˜è®¤ä¸Šé™
                    shortest = current_size
                    longest = 1333  # DETR é»˜è®¤ä¸Šé™
                
                # åŸºæœ¬æ•°å€¼éªŒè¯ï¼Œé¿å…æ— æ•ˆå°ºå¯¸
                try:
                    shortest = int(shortest)
                    longest = int(longest)
                    if shortest <= 0 or longest <= 0 or shortest > longest:
                        raise ValueError(f"Invalid size: shortest={shortest}, longest={longest}")
                except (TypeError, ValueError) as e:
                    logger.warning(
                        f"Progressive Resizing è·³è¿‡ï¼šæ— æ•ˆçš„å°ºå¯¸é…ç½® "
                        f"(shortest={shortest}, longest={longest}): {e}"
                    )
                    continue  # è·³è¿‡è¯¥ epoch çš„ resizing
                
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ transformers API
                # æ—§ç‰ˆæœ¬ï¼šsize + max_size
                # æ–°ç‰ˆæœ¬ï¼šsize={"shortest_edge": ..., "longest_edge": ...}
                try:
                    # å°è¯•æ–°ç‰ˆæœ¬ API (transformers >= 4.26)
                    image_processor.size = {"shortest_edge": shortest, "longest_edge": longest}
                except (TypeError, AttributeError):
                    # å›é€€åˆ°æ—§ç‰ˆæœ¬ API
                    image_processor.size = shortest
                    image_processor.max_size = longest
                
                logger.info(f"Progressive Resizing: shortest_edge={shortest}, max_size={longest}")
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_one_epoch(
            model=model,
            dataloader=train_loader,
            optimizer=optimizer,
            device=device,
            epoch=epoch,
            image_processor=image_processor,
            max_iters=max_iters,
            log_interval=log_interval,
            logger=logger,
            scaler=scaler,
            use_amp=use_amp,
            amp_dtype=amp_dtype,  # ä¼ é€’amp_dtype
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # éªŒè¯ï¼ˆå®šæœŸè¿›è¡Œï¼‰
        val_metrics = {}
        if epoch % eval_interval == 0:
            logger.info(f"\n{'='*60}")
            logger.info("å¼€å§‹éªŒè¯...")
            logger.info(f"{'='*60}")
            val_metrics = evaluate(
                model=model,
                dataloader=val_loader,
                device=device,
                coco_gt=coco_gt,
                logger=logger,
                score_threshold=0.05,
                image_processor=image_processor,
                config=config,
            )
            logger.info(f"éªŒè¯ç»“æœ: mAP={val_metrics.get('mAP', 0):.4f}, "
                       f"mAP@50={val_metrics.get('mAP_50', 0):.4f}, "
                       f"mAP@75={val_metrics.get('mAP_75', 0):.4f}")
        
        # è®°å½•æŒ‡æ ‡
        metrics = {
            'loss': avg_loss,
            'lr': current_lr,
        }
        metrics.update(val_metrics)  # æ·»åŠ éªŒè¯æŒ‡æ ‡
        
        metrics_logger.log(metrics, step=epoch, epoch=epoch)
        
        logger.info(f"Epoch {epoch} å®Œæˆ - Avg Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
        
        # ä¿å­˜checkpointï¼ˆå®Œæ•´çŠ¶æ€ï¼‰
        if epoch % save_interval == 0:
            save_checkpoint(
                model=model,
                optimizer=optimizer,
                epoch=epoch,
                step=epoch * len(train_loader),
                metrics=metrics,
                output_dir=output_dir,
                filename=f"checkpoint_epoch_{epoch}.pth",
                scheduler=scheduler,
                scaler=scaler,
                best_metric=best_map if best_map > 0 else None,
                best_loss=best_loss if best_loss < float('inf') else None,
                save_rng_state=True,
            )
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯mAPï¼Œå¦‚æœæ²¡æœ‰éªŒè¯åˆ™ä½¿ç”¨è®­ç»ƒlossï¼‰
        current_map = val_metrics.get('mAP', 0)
        if current_map > 0:  # æœ‰éªŒè¯ç»“æœæ—¶ä½¿ç”¨mAP
            if current_map > best_map:
                best_map = current_map
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³mAP: {best_map:.4f}")
                save_checkpoint(
                    model=model,
                    optimizer=optimizer,
                    epoch=epoch,
                    step=epoch * len(train_loader),
                    metrics=metrics,
                    output_dir=output_dir,
                    filename="best.pth",
                    is_best=True,
                    scheduler=scheduler,
                    scaler=scaler,
                    best_metric=best_map,
                    best_loss=best_loss if best_loss < float('inf') else None,
                    save_rng_state=True,
                )
        else:  # æ²¡æœ‰éªŒè¯æ—¶ä½¿ç”¨è®­ç»ƒloss
            if avg_loss < best_loss:
                best_loss = avg_loss
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³Loss: {best_loss:.4f}")
                save_checkpoint(
                    model=model,
                    optimizer=optimizer,
                    epoch=epoch,
                    step=epoch * len(train_loader),
                    metrics=metrics,
                    output_dir=output_dir,
                    filename="best.pth",
                    is_best=True,
                    scheduler=scheduler,
                    scaler=scaler,
                    best_metric=None,  # ä½¿ç”¨lossæ—¶ä¸è®°å½•best_metric
                    best_loss=best_loss,
                    save_rng_state=True,
                )
        
        # å¦‚æœè®¾ç½®äº†max_itersä¸”å·²è¾¾åˆ°é¢„æœŸepochæ•°ï¼Œåœæ­¢è®­ç»ƒ
        # æ³¨æ„ï¼šåªæœ‰åœ¨max_iterså¾ˆå°æ—¶æ‰æå‰åœæ­¢ï¼ˆçœŸæ­£çš„å†’çƒŸæµ‹è¯•ï¼‰
        if max_iters and max_iters <= 200 and epoch >= 2:
            logger.info(f"å†’çƒŸæµ‹è¯•æ¨¡å¼ï¼šå·²å®Œæˆ {epoch} ä¸ªepochï¼Œåœæ­¢è®­ç»ƒ")
            break
    
    # å¦‚æœå›  start_epoch > max_epochs è·³è¿‡äº†è®­ç»ƒï¼Œä¸ä¿å­˜ last.pth
    if skip_training:
        logger.warning("âš ï¸  å·²è·³è¿‡è®­ç»ƒï¼Œä¸ä¿å­˜ last.pth ä»¥é¿å…è¦†ç›–å·²æœ‰æ¨¡å‹")
        logger.info(f"\n{'='*60}")
        logger.info("è®­ç»ƒå·²ç»“æŸï¼ˆæœªæ‰§è¡Œæ–° epochï¼‰")
        logger.info(f"{'='*60}")
        return
    
    # ç¡®ä¿ epoch å’Œ metrics å§‹ç»ˆæœ‰å®šä¹‰ï¼ˆé¿å…ç©ºå¾ªç¯å´©æºƒï¼‰
    if 'epoch' not in locals():
        epoch = start_epoch - 1
    if 'metrics' not in locals():
        # å¤„ç† best_map ä¸º None çš„æƒ…å†µï¼ˆé¿å… metrics åŒ…å« None å€¼ï¼‰
        safe_best_map = best_map if best_map is not None else 0.0
        metrics = {'loss': 0.0, 'mAP': safe_best_map}
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_checkpoint(
        model=model,
        optimizer=optimizer,
        epoch=epoch,
        step=epoch * len(train_loader) if epoch > 0 else 0,
        metrics=metrics,
        output_dir=output_dir,
        filename="last.pth",
        scheduler=scheduler,
        scaler=scaler,
        best_metric=best_map if best_map > 0 else None,
        best_loss=best_loss if best_loss < float('inf') else None,
        save_rng_state=True,
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼")
    logger.info(f"   æ€»è€—æ—¶: {elapsed_time / 60:.2f} åˆ†é’Ÿ")
    
    # æ˜¾ç¤ºæœ€ä½³æŒ‡æ ‡ï¼ˆä¼˜å…ˆæ˜¾ç¤ºmAPï¼Œå¦åˆ™æ˜¾ç¤ºLossï¼‰
    if best_map > 0:
        logger.info(f"   æœ€ä½³mAP: {best_map:.4f}")
    elif best_loss < float('inf'):
        logger.info(f"   æœ€ä½³Loss: {best_loss:.4f}")
    else:
        logger.info(f"   æœ€ä½³æŒ‡æ ‡: æœªè®°å½•")
    
    logger.info(f"   è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"{'='*60}\n")
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ€»è€—æ—¶: {elapsed_time / 60:.2f} åˆ†é’Ÿ")
    
    # æ˜¾ç¤ºæœ€ä½³æŒ‡æ ‡ï¼ˆä¼˜å…ˆæ˜¾ç¤ºmAPï¼Œå¦åˆ™æ˜¾ç¤ºLossï¼‰
    if best_map > 0:
        print(f"   æœ€ä½³mAP: {best_map:.4f}")
    elif best_loss < float('inf'):
        print(f"   æœ€ä½³Loss: {best_loss:.4f}")
    else:
        print(f"   æœ€ä½³æŒ‡æ ‡: æœªè®°å½•")
    
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒDETRæ¨¡å‹ç”¨äºäº¤é€šåœºæ™¯ç›®æ ‡æ£€æµ‹"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/detr_baseline.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰",
    )
    parser.add_argument(
        "--max-iter",
        type=int,
        default=None,
        help="æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºå†’çƒŸæµ‹è¯•ï¼‰",
    )
    parser.add_argument(
        "--eval-interval",
        type=int,
        default=None,
        help="è¯„ä¼°é—´éš”",
    )
    parser.add_argument(
        "--save-interval",
        type=int,
        default=None,
        help="ä¿å­˜é—´éš”",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="ä»checkpointæ¢å¤è®­ç»ƒ",
    )
    parser.add_argument(
        "--subset-size",
        type=int,
        default=None,
        help="å­é›†å¤§å°ï¼ˆç”¨äºå¿«é€ŸéªŒè¯æˆ–å°æ ·æœ¬è¿‡æ‹Ÿåˆï¼‰",
    )
    parser.add_argument(
        "--overfit",
        action="store_true",
        help="è¿‡æ‹Ÿåˆæ¨¡å¼ï¼ˆå…³é—­æ•°æ®å¢å¼ºï¼Œå›ºå®šéšæœºç§å­ï¼‰",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = load_config(args.config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.max_iter:
        config['training']['max_iters'] = args.max_iter
    if args.eval_interval:
        config['training']['eval_interval'] = args.eval_interval
    if args.save_interval:
        config['training']['save_interval'] = args.save_interval
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_output_dir(config, args)
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜é…ç½®
    save_config(config, output_dir)
    
    # å¼€å§‹è®­ç»ƒ
    train(config, args)


if __name__ == "__main__":
    main()
