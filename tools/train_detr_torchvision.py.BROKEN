#!/usr/bin/env python3
"""
Torchvision DETR è®­ç»ƒè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
- ä½¿ç”¨ torchvision.io.read_imageï¼ˆC++ è§£ç ï¼‰
- æ”¯æŒ min_size=800, max_size=1333 çš„ resize
- ä¼˜åŒ– DataLoader å‚æ•°ï¼ˆworkers/prefetch/pin_memoryï¼‰
- æ”¯æŒ AMPã€checkpointã€è¯„ä¼°
"""

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from torchvision.io import read_image, ImageReadMode
import torchvision.transforms.functional as F

# æ³¨æ„ï¼šå½“å‰ç¯å¢ƒçš„ torchvision ä¸åŒ…å« DETRï¼Œä½¿ç”¨ transformers åº“
from transformers import DetrForObjectDetection, DetrConfig


class CocoDetrDataset(torch.utils.data.Dataset):
    """COCO æ ¼å¼æ•°æ®é›†ï¼ˆä½¿ç”¨ C++ å›¾åƒè§£ç ï¼‰"""
    
    def __init__(
        self,
        img_root: str,
        ann_file: str,
        min_size: int = 800,
        max_size: int = 1333,
        is_train: bool = True,
    ):
        self.root = Path(img_root)
        self.coco = COCO(str(ann_file))
        self.ids = list(sorted(self.coco.imgs.keys()))
        
        # ç±»åˆ«æ˜ å°„åˆ°è¿ç»­ [0..N-1]
        cat_ids = sorted(self.coco.getCatIds())
        self.cat_id_map = {cat_id: i for i, cat_id in enumerate(cat_ids)}
        self.num_classes = len(cat_ids)
        
        self.min_size = min_size
        self.max_size = max_size
        self.is_train = is_train

    def __len__(self):
        return len(self.ids)

    def _resize(self, image: torch.Tensor, target: Dict) -> Tuple[torch.Tensor, Dict]:
        """ä¿æŒçºµæ¨ªæ¯”çš„ resizeï¼ˆmin_size/max_size ç­–ç•¥ï¼‰"""
        c, h, w = image.shape
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        min_original_size = float(min(h, w))
        max_original_size = float(max(h, w))
        
        if max_original_size / min_original_size * self.min_size > self.max_size:
            size = int(round(self.max_size * min_original_size / max_original_size))
        else:
            size = self.min_size
        
        scale_factor = size / min_original_size
        
        # Resize å›¾åƒ
        new_h = int(h * scale_factor)
        new_w = int(w * scale_factor)
        image = F.resize(image, [new_h, new_w])
        
        # è°ƒæ•´ bbox
        if "boxes" in target and len(target["boxes"]) > 0:
            boxes = target["boxes"]
            boxes = boxes * scale_factor
            target["boxes"] = boxes
        
        if "area" in target:
            target["area"] = target["area"] * (scale_factor ** 2)
        
        target["size"] = torch.tensor([new_h, new_w])
        target["orig_size"] = torch.tensor([h, w])
        
        return image, target

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = self.root / img_info["file_name"]

        # C++ è§£ç ï¼ˆtorchvision.ioï¼‰
        img = read_image(str(img_path), mode=ImageReadMode.RGB)
        img = img.float() / 255.0  # [0, 1]

        # åŠ è½½æ ‡æ³¨
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)

        boxes = []
        labels = []
        areas = []
        iscrowd = []

        for ann in anns:
            x, y, w, h = ann["bbox"]
            boxes.append([x, y, x + w, y + h])  # xyxy
            labels.append(self.cat_id_map[ann["category_id"]])
            areas.append(ann.get("area", w * h))
            iscrowd.append(ann.get("iscrowd", 0))

        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
            areas = torch.zeros((0,), dtype=torch.float32)
            iscrowd = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            labels = torch.tensor(labels, dtype=torch.int64)
            areas = torch.tensor(areas, dtype=torch.float32)
            iscrowd = torch.tensor(iscrowd, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([img_id]),
            "area": areas,
            "iscrowd": iscrowd,
        }

        # Resizeï¼ˆä¿æŒçºµæ¨ªæ¯”ï¼‰
        img, target = self._resize(img, target)

        return img, target


def collate_fn(batch):
    """è‡ªå®šä¹‰ collateï¼Œä¿æŒåˆ—è¡¨æ ¼å¼"""
    images, targets = zip(*batch)
    return list(images), list(targets)


def build_model(num_classes: int, pretrained_backbone: bool = True):
    """æ„å»º DETR æ¨¡å‹ï¼ˆä½¿ç”¨ transformers åº“ï¼‰"""
    # ä½¿ç”¨ transformers çš„ DETR å®ç°
    # æ³¨æ„ï¼šå½“å‰ç¯å¢ƒ torchvision 0.24.1+cpu ä¸åŒ…å« DETR
    config = DetrConfig(
        num_labels=num_classes,
        num_queries=100,
    )
    
    if pretrained_backbone:
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼ˆä¼šä¸‹è½½æƒé‡ï¼‰
        model = DetrForObjectDetection.from_pretrained(
            "facebook/detr-resnet-50",
            num_labels=num_classes,
            ignore_mismatched_sizes=True,
        )
    else:
        # ä»å¤´è®­ç»ƒ
        model = DetrForObjectDetection(config)
    
    return model


def save_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    iteration: int,
    best_map: float,
    output_dir: Path,
    is_best: bool = False,
):
    """ä¿å­˜ checkpoint"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "epoch": epoch,
        "iteration": iteration,
        "best_map": best_map,
    }
    
    # ä¿å­˜ last.pth
    torch.save(checkpoint, output_dir / "last.pth")
    
    # ä¿å­˜ best.pth
    if is_best:
        torch.save(checkpoint, output_dir / "best.pth")
        print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: mAP={best_map:.4f}")


def load_checkpoint(checkpoint_path: str, model: nn.Module, optimizer: torch.optim.Optimizer = None):
    """åŠ è½½ checkpoint"""
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model"])
    
    if optimizer is not None and "optimizer" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer"])
    
    epoch = checkpoint.get("epoch", 0)
    iteration = checkpoint.get("iteration", 0)
    best_map = checkpoint.get("best_map", 0.0)
    
    print(f"âœ… åŠ è½½ checkpoint: epoch={epoch}, iter={iteration}, best_mAP={best_map:.4f}")
    return epoch, iteration, best_map


@torch.no_grad()
def evaluate(
    model: nn.Module,
    data_loader: DataLoader,
    device: torch.device,
    coco_gt: COCO,
) -> Dict[str, float]:
    """COCO è¯„ä¼°"""
    model.eval()
    
    results = []
    
    for images, targets in data_loader:
        images = [img.to(device, non_blocking=True) for img in images]
        
        outputs = model(images)
        
        # è§£æè¾“å‡º
        for i, output in enumerate(outputs):
            image_id = targets[i]["image_id"].item()
            boxes = output["boxes"].cpu()
            scores = output["scores"].cpu()
            labels = output["labels"].cpu()
            
            for box, score, label in zip(boxes, scores, labels):
                x1, y1, x2, y2 = box.tolist()
                results.append({
                    "image_id": image_id,
                    "category_id": label.item(),
                    "bbox": [x1, y1, x2 - x1, y2 - y1],  # xywh
                    "score": score.item(),
                })
    
    if len(results) == 0:
        print("âš ï¸ æ— æ£€æµ‹ç»“æœ")
        return {"mAP": 0.0, "AP_small": 0.0}
    
    # COCO è¯„ä¼°
    coco_dt = coco_gt.loadRes(results)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType="bbox")
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    metrics = {
        "mAP": coco_eval.stats[0],
        "AP50": coco_eval.stats[1],
        "AP75": coco_eval.stats[2],
        "AP_small": coco_eval.stats[3],
        "AP_medium": coco_eval.stats[4],
        "AP_large": coco_eval.stats[5],
    }
    
    model.train()
    return metrics


def train_one_epoch(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    data_loader: DataLoader,
    device: torch.device,
    epoch: int,
    use_amp: bool,
    amp_dtype: torch.dtype,
    print_freq: int = 50,
) -> Dict[str, float]:
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    
    total_loss = 0.0
    start_time = time.time()
    
    for step, (images, targets) in enumerate(data_loader, start=1):
        # non_blocking ä¼ è¾“
        images = [img.to(device, non_blocking=True) for img in images]
        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]
        
        optimizer.zero_grad(set_to_none=True)
        
        # AMP forward
        with torch.autocast(device_type=device.type, dtype=amp_dtype, enabled=use_amp):
            loss_dict = model(images, targets)
            loss = sum(loss_dict.values())
        
        # Backward
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        # æ‰“å°æ—¥å¿—
        if step % print_freq == 0:
            torch.cuda.synchronize()
            elapsed = time.time() - start_time
            it_s = step / elapsed
            avg_loss = total_loss / step
            
            print(f"Epoch [{epoch}] Step [{step}/{len(data_loader)}] "
                  f"Loss: {loss.item():.4f} (avg: {avg_loss:.4f}) | "
                  f"Speed: {it_s:.2f} it/s")
    
    avg_loss = total_loss / len(data_loader)
    return {"loss": avg_loss}


def main():
    parser = argparse.ArgumentParser(description="Torchvision DETR è®­ç»ƒè„šæœ¬")
    
    # æ•°æ®è·¯å¾„
    parser.add_argument("--train-img", type=str, required=True, help="è®­ç»ƒå›¾åƒç›®å½•")
    parser.add_argument("--train-ann", type=str, required=True, help="è®­ç»ƒæ ‡æ³¨æ–‡ä»¶")
    parser.add_argument("--val-img", type=str, help="éªŒè¯å›¾åƒç›®å½•")
    parser.add_argument("--val-ann", type=str, help="éªŒè¯æ ‡æ³¨æ–‡ä»¶")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--num-classes", type=int, default=3, help="ç±»åˆ«æ•°")
    parser.add_argument("--pretrained-backbone", action="store_true", help="ä½¿ç”¨é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch-size", type=int, default=16, help="Batch size")
    parser.add_argument("--num-epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=5e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight-decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    
    # DataLoader ä¼˜åŒ–
    parser.add_argument("--num-workers", type=int, default=12, help="DataLoader workers")
    parser.add_argument("--prefetch-factor", type=int, default=2, help="é¢„å–å› å­")
    
    # å›¾åƒå°ºå¯¸
    parser.add_argument("--min-size", type=int, default=800, help="æœ€å°è¾¹å°ºå¯¸")
    parser.add_argument("--max-size", type=int, default=1333, help="æœ€å¤§è¾¹å°ºå¯¸")
    
    # è®­ç»ƒç­–ç•¥
    parser.add_argument("--amp", action="store_true", help="å¯ç”¨ AMP æ··åˆç²¾åº¦")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--subset", type=int, help="ä½¿ç”¨æ•°æ®å­é›†ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    
    # Checkpoint
    parser.add_argument("--output-dir", type=str, default="outputs/detr_torchvision", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resume", type=str, help="æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„")
    parser.add_argument("--eval-interval", type=int, default=5, help="è¯„ä¼°é—´éš”ï¼ˆepochï¼‰")
    parser.add_argument("--print-freq", type=int, default=50, help="æ‰“å°é¢‘ç‡ï¼ˆstepï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(output_dir / "config.json", "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device)
    
    # ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision("high")
    
    print("=" * 80)
    print("ğŸš€ Torchvision DETR è®­ç»ƒ")
    print("=" * 80)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è®¾å¤‡: {device}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Workers: {args.num_workers}")
    print(f"å›¾åƒå°ºå¯¸: min={args.min_size}, max={args.max_size}")
    print(f"AMP: {args.amp}")
    print("=" * 80)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = CocoDetrDataset(
        args.train_img,
        args.train_ann,
        min_size=args.min_size,
        max_size=args.max_size,
        is_train=True,
    )
    
    if args.subset:
        train_dataset = Subset(train_dataset, range(min(args.subset, len(train_dataset))))
        print(f"ğŸ“Š ä½¿ç”¨è®­ç»ƒå­é›†: {len(train_dataset)} å¼ å›¾åƒ")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=args.prefetch_factor,
    )
    
    # éªŒè¯é›†ï¼ˆå¯é€‰ï¼‰
    val_loader = None
    coco_gt = None
    if args.val_img and args.val_ann:
        val_dataset = CocoDetrDataset(
            args.val_img,
            args.val_ann,
            min_size=args.min_size,
            max_size=args.max_size,
            is_train=False,
        )
        
        if args.subset:
            val_dataset = Subset(val_dataset, range(min(args.subset // 4, len(val_dataset))))
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
            prefetch_factor=args.prefetch_factor,
        )
        
        coco_gt = COCO(args.val_ann)
        print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ")
    
    # æ„å»ºæ¨¡å‹
    print(f"ğŸ“¦ æ„å»ºæ¨¡å‹: num_classes={args.num_classes}")
    model = build_model(args.num_classes, pretrained_backbone=args.pretrained_backbone)
    model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # AMP é…ç½®
    amp_dtype = torch.bfloat16 if args.amp and torch.cuda.is_bf16_supported() else torch.float16
    use_amp = args.amp and device.type == "cuda"
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_map = 0.0
    
    if args.resume:
        start_epoch, _, best_map = load_checkpoint(args.resume, model, optimizer)
    
    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    
    metrics_log = []
    
    for epoch in range(start_epoch, args.num_epochs):
        epoch_start = time.time()
        
        # è®­ç»ƒ
        train_metrics = train_one_epoch(
            model, optimizer, train_loader, device, epoch + 1,
            use_amp, amp_dtype, args.print_freq
        )
        
        epoch_time = time.time() - epoch_start
        
        # è¯„ä¼°
        eval_metrics = {}
        is_best = False
        
        if val_loader and ((epoch + 1) % args.eval_interval == 0 or epoch == args.num_epochs - 1):
            print(f"\nğŸ“Š è¯„ä¼° Epoch {epoch + 1}...")
            eval_metrics = evaluate(model, val_loader, device, coco_gt)
            
            current_map = eval_metrics["mAP"]
            if current_map > best_map:
                best_map = current_map
                is_best = True
        
        # ä¿å­˜ checkpoint
        save_checkpoint(
            model, optimizer, epoch + 1, 0, best_map,
            output_dir, is_best
        )
        
        # è®°å½•æ—¥å¿—
        log_entry = {
            "epoch": epoch + 1,
            "train_loss": train_metrics["loss"],
            "epoch_time": epoch_time,
            **eval_metrics,
        }
        metrics_log.append(log_entry)
        
        # ä¿å­˜æ—¥å¿—
        with open(output_dir / "metrics.json", "w") as f:
            json.dump(metrics_log, f, indent=2)
        
        print(f"\n{'=' * 80}")
        print(f"Epoch [{epoch + 1}/{args.num_epochs}] å®Œæˆ | è€—æ—¶: {epoch_time:.1f}s")
        print(f"è®­ç»ƒ Loss: {train_metrics['loss']:.4f}")
        if eval_metrics:
            print(f"éªŒè¯ mAP: {eval_metrics['mAP']:.4f} | AP_small: {eval_metrics['AP_small']:.4f}")
            print(f"æœ€ä½³ mAP: {best_map:.4f}")
        print(f"{'=' * 80}\n")
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
