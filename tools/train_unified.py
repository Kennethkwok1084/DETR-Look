#!/usr/bin/env python3
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬
æ”¯æŒ DETR (HF) å’Œ Deformable DETR (å®˜æ–¹) åŒæ•°æ®æµ
æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„ model.type è‡ªåŠ¨é€‰æ‹©
"""

import argparse
import os
import sys
from pathlib import Path

import torch
import yaml
from torch.amp import GradScaler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models import build_model, build_image_processor
from utils import setup_logger, save_checkpoint, load_checkpoint, train_one_epoch


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def build_dataloader_for_model(config, image_set='train'):
    """
    æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºå¯¹åº”çš„æ•°æ®åŠ è½½å™¨
    
    Args:
        config: é…ç½®å­—å…¸
        image_set: 'train' æˆ– 'val'
    
    Returns:
        (dataloader, dataset) å…ƒç»„
    """
    model_type = config.get('model', {}).get('type', 'detr').lower()
    
    if model_type == 'deformable_detr' or model_type == 'deformable-detr':
        # Deformable DETR: ä½¿ç”¨å®˜æ–¹æ•°æ®æµ
        from dataset.deformable_dataset import build_deformable_dataloader
        return build_deformable_dataloader(config, image_set)
    else:
        # DETR: ä½¿ç”¨ HF æ•°æ®æµ
        from dataset import build_dataloader
        
        image_processor = build_image_processor(config)
        dataloader = build_dataloader(
            config=config,
            image_set=image_set,
            image_processor=image_processor
        )
        # build_dataloader åªè¿”å› dataloaderï¼Œæˆ‘ä»¬éœ€è¦è¿”å› (dataloader, dataset)
        return dataloader, dataloader.dataset


def build_optimizer(model, config):
    """æ„å»ºä¼˜åŒ–å™¨"""
    train_config = config['training']
    opt_config = train_config['optimizer']
    
    if opt_config['type'] == 'AdamW':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=opt_config['lr'],
            weight_decay=opt_config.get('weight_decay', 0.0001),
            betas=tuple(opt_config.get('betas', [0.9, 0.999]))
        )
    elif opt_config['type'] == 'Adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=opt_config['lr'],
            weight_decay=opt_config.get('weight_decay', 0.0)
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {opt_config['type']}")
    
    return optimizer


def build_scheduler(optimizer, config):
    """æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    train_config = config['training']
    
    if 'lr_scheduler' not in train_config:
        return None
    
    sch_config = train_config['lr_scheduler']
    
    if sch_config['type'] == 'StepLR':
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=sch_config['step_size'],
            gamma=sch_config.get('gamma', 0.1)
        )
    elif sch_config['type'] == 'MultiStepLR':
        scheduler = torch.optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=sch_config['milestones'],
            gamma=sch_config.get('gamma', 0.1)
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨: {sch_config['type']}")
    
    return scheduler


def main(args):
    """ä¸»è®­ç»ƒæµç¨‹"""
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆæ”¯æŒå¤šç§é…ç½®é”®ï¼‰
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path(
            config['training'].get('output_dir') or 
            config.get('output', {}).get('base_dir', 'outputs') + '/' + 
            config.get('output', {}).get('experiment_name', 'experiment')
        )
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger('train', output_dir / 'train.log')
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
    logger.info(f"é…ç½®æ–‡ä»¶: {args.config}")
    logger.info(f"æ¨¡å‹ç±»å‹: {config.get('model', {}).get('type', 'detr')}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜é…ç½®
    with open(output_dir / 'config.yaml', 'w') as f:
        yaml.dump(config, f)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"è®¾å¤‡: {device}")
    
    # æ„å»ºæ¨¡å‹
    logger.info("æ„å»ºæ¨¡å‹...")
    model = build_model(config)
    model.to(device)
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    logger.info("æ„å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, train_dataset = build_dataloader_for_model(config, 'train')
    
    # æ„å»ºä¼˜åŒ–å™¨
    optimizer = build_optimizer(model, config)
    logger.info(f"ä¼˜åŒ–å™¨: {config['training']['optimizer']['type']}")
    
    # æ„å»ºè°ƒåº¦å™¨
    scheduler = build_scheduler(optimizer, config)
    if scheduler:
        logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {config['training']['lr_scheduler']['type']}")
    
    # æ··åˆç²¾åº¦ï¼ˆæ”¯æŒå¤šç§é…ç½®é”®ï¼Œä¼˜å…ˆçº§ï¼štraining.amp > training.use_amp > amp.enabledï¼‰
    use_amp = config['training'].get('amp')
    if use_amp is None:
        use_amp = config['training'].get('use_amp')
    if use_amp is None:
        use_amp = config.get('amp', {}).get('enabled', False)
    scaler = GradScaler('cuda') if use_amp else None
    if use_amp:
        logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    start_epoch = 1
    if args.resume:
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume}")
        checkpoint = load_checkpoint(args.resume, model, optimizer, scheduler)
        start_epoch = checkpoint.get('epoch', 0) + 1
    
    # è®­ç»ƒé…ç½®ï¼ˆæ”¯æŒå¤šç§é”®åï¼‰
    num_epochs = config['training'].get('num_epochs') or config['training'].get('max_epochs', 50)
    log_interval = config['training'].get('log_interval', 50)
    save_interval = config['training'].get('save_interval', 5)
    
    # è®­ç»ƒå¾ªç¯
    logger.info(f"å¼€å§‹è®­ç»ƒ: Epoch {start_epoch} -> {num_epochs}")
    
    for epoch in range(start_epoch, num_epochs + 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"Epoch {epoch}/{num_epochs}")
        logger.info(f"{'='*60}")
        
        # è®­ç»ƒä¸€ä¸ª epoch
        avg_loss = train_one_epoch(
            model=model,
            dataloader=train_loader,
            optimizer=optimizer,
            device=device,
            epoch=epoch,
            logger=logger,
            config=config,
            log_interval=log_interval,
            use_amp=use_amp,
            scaler=scaler
        )
        
        logger.info(f"Epoch {epoch} å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler:
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            logger.info(f"å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % save_interval == 0 or epoch == num_epochs:
            checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'
            save_checkpoint(
                model=model,
                optimizer=optimizer,
                epoch=epoch,
                step=0,  # ç®€åŒ–ç‰ˆä¸è·Ÿè¸ª step
                metrics={'avg_loss': avg_loss},
                output_dir=output_dir,
                filename=checkpoint_filename,
                scheduler=scheduler,
                scaler=scaler
            )
            logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {output_dir / checkpoint_filename}")
    
    logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='DETR/Deformable DETR ç»Ÿä¸€è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ï¼‰')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    main(args)
