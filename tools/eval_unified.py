#!/usr/bin/env python3
"""
ç»Ÿä¸€è¯„ä¼°è„šæœ¬
æ”¯æŒ DETR (HF) å’Œ Deformable DETR (å®˜æ–¹) åŒæ•°æ®æµ
"""

import argparse
import json
import sys
from pathlib import Path

import torch
import yaml
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models import build_model, build_image_processor
from utils import load_checkpoint, setup_logger


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def build_eval_dataloader(config, eval_set='val'):
    """æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨"""
    model_type = config.get('model', {}).get('type', 'detr').lower()
    
    if model_type == 'deformable_detr' or model_type == 'deformable-detr':
        # Deformable DETR: å®˜æ–¹æ•°æ®æµ
        from dataset.deformable_dataset import build_deformable_dataloader
        return build_deformable_dataloader(config, eval_set)
    else:
        # DETR: HF æ•°æ®æµ
        from dataset import build_dataloader
        image_processor = build_image_processor(config)
        dataloader = build_dataloader(
            config=config,
            image_set=eval_set,
            image_processor=image_processor
        )
        # è¿”å› (dataloader, dataset) å…ƒç»„
        return dataloader, dataloader.dataset


@torch.no_grad()
def evaluate_detr(model, dataloader, device, coco_gt, logger, config, score_threshold=0.05):
    """
    è¯„ä¼° DETR æ¨¡å‹ï¼ˆHF æ•°æ®æµï¼‰
    
    Args:
        model: DETR æ¨¡å‹
        dataloader: HF æ ¼å¼æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        coco_gt: COCO ground truth
        logger: æ—¥å¿—å™¨
        config: é…ç½®å­—å…¸ï¼ˆç”¨äºæ„å»º image_processorï¼‰
        score_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        metrics dict
    """
    from transformers import DetrImageProcessor
    
    model.eval()
    results = []
    
    logger.info("è¯„ä¼° DETR (HF æ•°æ®æµ)...")
    
    # ä» config æ„å»º image processor
    image_processor = build_image_processor(config)
    if image_processor is None:
        raise ValueError("DETR è¯„ä¼°éœ€è¦ image_processor")
    
    for batch in tqdm(dataloader, desc="Evaluating DETR"):
        pixel_values = batch['pixel_values'].to(device)
        pixel_mask = batch['pixel_mask'].to(device) if 'pixel_mask' in batch else None
        
        # ä¿ç•™åŸå§‹ targetsï¼ˆåŒ…å« image_idï¼‰
        # æ³¨æ„ï¼šbatch['labels'] æ˜¯ HF processor å¤„ç†åçš„ï¼Œå¯èƒ½ä¸å« image_id
        # ä¼˜å…ˆä½¿ç”¨åŸå§‹ batch['targets']ï¼Œfallback åˆ° batch['labels']
        original_targets = batch.get('targets', batch.get('labels', []))
        
        # åŸå›¾ PIL imagesï¼ˆç”¨äºè·å–å°ºå¯¸ï¼‰
        images = batch.get('images', None)
        if images is None:
            # å¦‚æœæ²¡æœ‰åŸå›¾ï¼Œä» targets è·å–å°ºå¯¸
            target_sizes = torch.tensor([[t.get('orig_size', t.get('size', [800, 800]))[0], 
                                         t.get('orig_size', t.get('size', [800, 800]))[1]] 
                                        for t in original_targets]).to(device)
        else:
            target_sizes = torch.tensor([img.size[::-1] for img in images]).to(device)
        
        # æ¨ç†
        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)
        
        # åå¤„ç†
        processed_outputs = image_processor.post_process_object_detection(
            outputs,
            threshold=score_threshold,
            target_sizes=target_sizes
        )
        
        # è½¬æ¢ä¸º COCO æ ¼å¼ï¼ˆä½¿ç”¨åŸå§‹ targets ç¡®ä¿æœ‰ image_idï¼‰
        for output, target in zip(processed_outputs, original_targets):
            # ç¡®ä¿ image_id å­˜åœ¨
            if 'image_id' not in target:
                raise ValueError(f"Target ç¼ºå°‘ image_idï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½å™¨æ˜¯å¦ä¿ç•™åŸå§‹ targets")
            image_id = target['image_id'].item() if torch.is_tensor(target['image_id']) else target['image_id']
            
            scores = output['scores']
            labels = output['labels']
            boxes = output['boxes']  # xyxy æ ¼å¼
            
            for score, label, box in zip(scores, labels, boxes):
                x1, y1, x2, y2 = box.tolist()
                results.append({
                    'image_id': image_id,
                    'category_id': label.item(),
                    'bbox': [x1, y1, x2 - x1, y2 - y1],  # xywh
                    'score': score.item(),
                })
    
    return compute_coco_metrics(results, coco_gt, logger)


@torch.no_grad()
def evaluate_deformable(model, dataloader, device, coco_gt, logger, score_threshold=0.05):
    """
    è¯„ä¼° Deformable DETR æ¨¡å‹ï¼ˆå®˜æ–¹æ•°æ®æµï¼‰
    
    Args:
        model: Deformable DETR æ¨¡å‹å°è£…
        dataloader: å®˜æ–¹æ ¼å¼æ•°æ®åŠ è½½å™¨ï¼ˆè¿”å› NestedTensor, targetsï¼‰
        device: è®¾å¤‡
        coco_gt: COCO ground truth
        logger: æ—¥å¿—å™¨
        score_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        metrics dict
    """
    import sys
    from pathlib import Path
    
    # æ·»åŠ  third_party è·¯å¾„
    _third_party_path = Path(__file__).parent.parent / "third_party" / "deformable_detr"
    if str(_third_party_path) not in sys.path:
        sys.path.insert(0, str(_third_party_path))
    
    from util.misc import NestedTensor
    
    model.eval()
    results = []
    
    logger.info("è¯„ä¼° Deformable DETR (å®˜æ–¹æ•°æ®æµ)...")
    
    for samples, targets in tqdm(dataloader, desc="Evaluating Deformable"):
        # ç§»åˆ°è®¾å¤‡
        if isinstance(samples, NestedTensor):
            samples = NestedTensor(samples.tensors.to(device), samples.mask.to(device))
        else:
            samples = samples.to(device)
        
        # æ¨ç†ï¼ˆä¸éœ€è¦ targetsï¼‰
        outputs = model(samples, targets=None)
        
        # è·å–åŸå›¾å°ºå¯¸ï¼ˆä» targetsï¼‰
        orig_target_sizes = torch.stack([t["orig_size"] for t in targets], dim=0).to(device)
        
        # åå¤„ç†ï¼ˆä½¿ç”¨å®˜æ–¹ PostProcessï¼‰
        processed_outputs = model.postprocess(outputs, orig_target_sizes)
        
        # è½¬æ¢ä¸º COCO æ ¼å¼
        for output, target in zip(processed_outputs, targets):
            image_id = target['image_id'].item() if torch.is_tensor(target['image_id']) else target['image_id'][0]
            
            scores = output['scores']
            labels = output['labels']
            boxes = output['boxes']  # xyxy æ ¼å¼ï¼ˆå·²è¿˜åŸåˆ°åŸå›¾ï¼‰
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦
            keep = scores > score_threshold
            scores = scores[keep]
            labels = labels[keep]
            boxes = boxes[keep]
            
            for score, label, box in zip(scores, labels, boxes):
                x1, y1, x2, y2 = box.tolist()
                
                # å…³é”®ï¼šå°†è¿ç»­ç´¢å¼•åæ˜ å°„å›åŸå§‹ COCO category_id
                # Deformable æ•°æ®é›†å°† category_id æ˜ å°„ä¸º [0, num_classes-1]
                # éœ€è¦åæ˜ å°„å›åŸå§‹ ID ä»¥åŒ¹é… COCO æ ‡æ³¨
                continuous_id = label.item()
                if hasattr(dataloader.dataset, 'dataset'):  # Subset åŒ…è£…
                    dataset = dataloader.dataset.dataset
                else:
                    dataset = dataloader.dataset
                
                if hasattr(dataset, 'continuous_to_cat_id'):
                    category_id = dataset.continuous_to_cat_id[continuous_id]
                else:
                    # DETR æˆ–æ— æ˜ å°„çš„æ•°æ®é›†ï¼Œç›´æ¥ä½¿ç”¨
                    category_id = continuous_id
                
                results.append({
                    'image_id': image_id,
                    'category_id': category_id,
                    'bbox': [x1, y1, x2 - x1, y2 - y1],  # xywh
                    'score': score.item(),
                })
    
    return compute_coco_metrics(results, coco_gt, logger)


def compute_coco_metrics(results, coco_gt, logger):
    """è®¡ç®— COCO æŒ‡æ ‡"""
    logger.info(f"ç”Ÿæˆäº† {len(results)} ä¸ªæ£€æµ‹ç»“æœ")
    
    if len(results) == 0:
        logger.warning("æ²¡æœ‰æ£€æµ‹ç»“æœï¼")
        return {}
    
    # ä½¿ç”¨ COCO API è¯„ä¼°
    logger.info("å¼€å§‹ COCO è¯„ä¼°...")
    coco_dt = coco_gt.loadRes(results)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–æŒ‡æ ‡
    metrics = {
        'mAP': coco_eval.stats[0],
        'mAP_50': coco_eval.stats[1],
        'mAP_75': coco_eval.stats[2],
        'mAP_small': coco_eval.stats[3],
        'mAP_medium': coco_eval.stats[4],
        'mAP_large': coco_eval.stats[5],
    }
    
    return metrics


def evaluate(model, dataloader, device, coco_gt, logger, config, score_threshold=0.05):
    """
    ç»Ÿä¸€è¯„ä¼°å…¥å£ï¼ˆè‡ªåŠ¨é€‰æ‹©æ•°æ®æµï¼‰
    """
    model_type = config.get('model', {}).get('type', 'detr').lower()
    
    if model_type == 'deformable_detr' or model_type == 'deformable-detr':
        return evaluate_deformable(model, dataloader, device, coco_gt, logger, score_threshold)
    else:
        return evaluate_detr(model, dataloader, device, coco_gt, logger, config, score_threshold)


def main():
    parser = argparse.ArgumentParser(description="DETR/Deformable DETR ç»Ÿä¸€è¯„ä¼°è„šæœ¬")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, required=True, help="checkpoint è·¯å¾„")
    parser.add_argument("--eval-set", type=str, default="val", choices=["train", "val"], help="è¯„ä¼°é›†")
    parser.add_argument("--output", type=str, help="ç»“æœè¾“å‡ºè·¯å¾„")
    parser.add_argument("--score-threshold", type=float, default=0.05, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    model_type = config.get('model', {}).get('type', 'detr')
    print(f"ğŸ”§ æ¨¡å‹ç±»å‹: {model_type}")
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    
    # æ—¥å¿—
    logger = setup_logger('eval')
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“¦ æ„å»ºæ•°æ®åŠ è½½å™¨...")
    dataloader, dataset = build_eval_dataloader(config, args.eval_set)
    
    # åŠ è½½ COCO ground truth
    if args.eval_set == 'train':
        ann_file = config['dataset']['train_ann']
    else:
        ann_file = config['dataset']['val_ann']
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ root_dir
    from pathlib import Path
    ann_file = Path(ann_file)
    if not ann_file.is_absolute():
        root_dir = config['dataset'].get('root_dir', '')
        if root_dir:
            ann_file = Path(root_dir) / ann_file
    
    coco_gt = COCO(str(ann_file))
    
    # æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    model = build_model(config)
    model = model.to(device)
    
    # åŠ è½½ checkpoint
    print(f"\nğŸ“‚ åŠ è½½ checkpoint: {args.checkpoint}")
    load_checkpoint(args.checkpoint, model, device=str(device))
    
    # è¯„ä¼°
    print("\nğŸ¯ å¼€å§‹è¯„ä¼°")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {args.score_threshold}")
    print("=" * 60)
    
    metrics = evaluate(model, dataloader, device, coco_gt, logger, config, args.score_threshold)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("=" * 60)
    for k, v in metrics.items():
        print(f"  {k}: {v:.4f}")
    print("=" * 60)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == '__main__':
    main()
