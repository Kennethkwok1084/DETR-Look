#!/usr/bin/env python3
"""
DETRæ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨pycocotoolsè®¡ç®—COCOæ ¼å¼çš„æ£€æµ‹æŒ‡æ ‡
"""

import argparse
import json
import sys
from pathlib import Path

import torch
import yaml
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from transformers import DetrImageProcessor
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dataset import build_dataloader
from models import build_detr_model
from utils import load_checkpoint, setup_logger


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


@torch.no_grad()
def evaluate(model, dataloader, device, coco_gt, logger, score_threshold=0.05, image_processor=None, config=None):
    """
    è¯„ä¼°æ¨¡å‹
    
    Args:
        model: DETRæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        coco_gt: COCO ground truthå¯¹è±¡
        logger: æ—¥å¿—å™¨
        score_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        image_processor: DETRå›¾åƒå¤„ç†å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœªæä¾›åˆ™ä»configæ„å»ºï¼‰
        config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œä»…åœ¨image_processor=Noneæ—¶éœ€è¦ï¼‰
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    
    results = []
    
    logger.info("å¼€å§‹è¯„ä¼°...")
    
    # åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if image_processor is None:
        if config is None:
            raise ValueError("å½“image_processor=Noneæ—¶ï¼Œå¿…é¡»æä¾›configå‚æ•°")
        # ä»é…ç½®ä¸­è¯»å–æ¨¡å‹åç§°ï¼Œä¿æŒä¸æ¨¡å‹ä¸€è‡´
        model_name = config['model']['name']
        if not model_name.startswith('facebook/'):
            model_name = f"facebook/{model_name}"
        logger.info(f"åˆå§‹åŒ–DetrImageProcessor: {model_name}")
        image_processor = DetrImageProcessor.from_pretrained(model_name)
    
    for images, targets in tqdm(dataloader, desc="Evaluating"):
        # imagesæ˜¯PIL.Imageåˆ—è¡¨ï¼Œtargetsæ˜¯COCOæ ¼å¼å­—å…¸åˆ—è¡¨
        
        # ä½¿ç”¨DetrImageProcessorå¤„ç†PILå›¾åƒ
        encoding = image_processor(images=images, return_tensors='pt')
        
        pixel_values = encoding['pixel_values'].to(device)
        pixel_mask = encoding['pixel_mask'].to(device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)
        
        # ä½¿ç”¨post_process_object_detectionè¿˜åŸé¢„æµ‹åˆ°åŸå›¾å°ºå¯¸
        # è·å–åŸå›¾å°ºå¯¸ï¼ˆä»åŸå§‹PILå›¾åƒï¼‰
        target_sizes = torch.tensor([img.size[::-1] for img in images]).to(device)  # (height, width)
        
        # post_processä¼šè‡ªåŠ¨è¿˜åŸåˆ°åŸå›¾å°ºå¯¸å¹¶è½¬æ¢ä¸ºxyxyæ ¼å¼
        processed_outputs = image_processor.post_process_object_detection(
            outputs,
            threshold=score_threshold,
            target_sizes=target_sizes
        )
        
        # è½¬æ¢ä¸ºCOCOæ ¼å¼
        for i, (output, target) in enumerate(zip(processed_outputs, targets)):
            image_id = target['image_id']
            
            # outputåŒ…å«: scores, labels, boxes (xyxyæ ¼å¼ï¼ŒåŸå›¾å°ºå¯¸)
            scores = output['scores']
            labels = output['labels']
            boxes = output['boxes']  # xyxyæ ¼å¼
            
            # è½¬æ¢ä¸ºCOCOçš„xywhæ ¼å¼
            for score, label, box in zip(scores, labels, boxes):
                x1, y1, x2, y2 = box.tolist()
                results.append({
                    'image_id': image_id,
                    'category_id': label.item(),
                    'bbox': [x1, y1, x2 - x1, y2 - y1],  # è½¬ä¸ºxywh
                    'score': score.item(),
                })
    
    logger.info(f"ç”Ÿæˆäº† {len(results)} ä¸ªæ£€æµ‹ç»“æœ")
    
    if len(results) == 0:
        logger.warning("æ²¡æœ‰æ£€æµ‹ç»“æœï¼")
        return {}
    
    # ä½¿ç”¨COCO APIè¯„ä¼°
    logger.info("å¼€å§‹COCOè¯„ä¼°...")
    coco_dt = coco_gt.loadRes(results)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–å…³é”®æŒ‡æ ‡
    metrics = {
        'mAP': coco_eval.stats[0],
        'mAP_50': coco_eval.stats[1],
        'mAP_75': coco_eval.stats[2],
        'mAP_small': coco_eval.stats[3],
        'mAP_medium': coco_eval.stats[4],
        'mAP_large': coco_eval.stats[5],
    }
    
    return metrics


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°DETRæ¨¡å‹")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="æ¨¡å‹checkpointè·¯å¾„",
    )
    parser.add_argument(
        "--eval-set",
        type=str,
        default="val",
        choices=["train", "val", "test"],
        help="è¯„ä¼°æ•°æ®é›†",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ç»“æœè¾“å‡ºè·¯å¾„",
    )
    parser.add_argument(
        "--score-threshold",
        type=float,
        default=0.05,
        help="æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.05ï¼‰",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config['device']['type'] if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger('eval')
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“¦ æ„å»ºæ•°æ®åŠ è½½å™¨")
    dataloader = build_dataloader(config, args.eval_set)
    
    # åŠ è½½COCO ground truth
    root = Path(config['dataset']['root_dir'])
    ann_file = root / config['dataset'][f'{args.eval_set}_ann']
    coco_gt = COCO(str(ann_file))
    
    # æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹")
    model = build_detr_model(config)
    model = model.to(device)
    
    # åŠ è½½checkpoint
    print(f"\nğŸ“‚ åŠ è½½checkpoint: {args.checkpoint}")
    load_checkpoint(args.checkpoint, model, device=str(device))
    
    # è¯„ä¼°
    print("\nğŸ¯ å¼€å§‹è¯„ä¼°")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {args.score_threshold}")
    print("="*60)
    metrics = evaluate(model, dataloader, device, coco_gt, logger, args.score_threshold, config=config)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("="*60)
    for k, v in metrics.items():
        print(f"  {k}: {v:.4f}")
    print("="*60)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == '__main__':
    main()
