#!/usr/bin/env python3
"""
DETRæ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨pycocotoolsè®¡ç®—COCOæ ¼å¼çš„æ£€æµ‹æŒ‡æ ‡
"""

import argparse
import json
import sys
from pathlib import Path

import torch
import yaml
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from transformers import DetrImageProcessor
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dataset import build_dataloader
from models import build_detr_model
from utils import load_checkpoint, setup_logger


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


@torch.no_grad()
def evaluate(model, dataloader, device, coco_gt, logger, score_threshold=0.05, image_processor=None):
    """
    è¯„ä¼°æ¨¡å‹
    
    Args:
        model: DETRæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        coco_gt: COCO ground truthå¯¹è±¡
        logger: æ—¥å¿—å™¨
        score_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        image_processor: DETRå›¾åƒå¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    
    results = []
    
    logger.info("å¼€å§‹è¯„ä¼°...")
    
    # åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if image_processor is None:
        image_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')
    
    for images, targets in tqdm(dataloader, desc="Evaluating"):
        # ä½¿ç”¨DetrImageProcessorå¤„ç†å¯å˜å°ºå¯¸å›¾åƒ
        images_pil = [img.cpu().numpy().transpose(1, 2, 0) for img in images]
        encoding = image_processor(images=images_pil, return_tensors='pt')
        
        pixel_values = encoding['pixel_values'].to(device)
        pixel_mask = encoding['pixel_mask'].to(device)
        
        # æ¨ç†
        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)
        
        # è§£æè¾“å‡º
        logits = outputs.logits  # [B, num_queries, num_classes+1]
        boxes = outputs.pred_boxes  # [B, num_queries, 4] (cxcywhæ ¼å¼ï¼Œå½’ä¸€åŒ–)
        
        # è½¬æ¢ä¸ºCOCOæ ¼å¼
        for i, target in enumerate(targets):
            image_id = target['image_id'].item()
            img_h, img_w = target['size'].tolist()
            
            # è·å–é¢„æµ‹
            scores = logits[i].softmax(-1)  # [num_queries, num_classes+1]
            max_scores, labels = scores[:, :-1].max(-1)  # æ’é™¤backgroundç±»
            
            # è½¬æ¢boxesæ ¼å¼: cxcywh -> xyxy -> xywh (COCOæ ¼å¼)
            pred_boxes = boxes[i]
            
            # cxcywh -> xyxy (åƒç´ åæ ‡)
            boxes_xyxy = torch.zeros_like(pred_boxes)
            boxes_xyxy[:, 0] = (pred_boxes[:, 0] - pred_boxes[:, 2] / 2) * img_w  # x1
            boxes_xyxy[:, 1] = (pred_boxes[:, 1] - pred_boxes[:, 3] / 2) * img_h  # y1
            boxes_xyxy[:, 2] = (pred_boxes[:, 0] + pred_boxes[:, 2] / 2) * img_w  # x2
            boxes_xyxy[:, 3] = (pred_boxes[:, 1] + pred_boxes[:, 3] / 2) * img_h  # y2
            
            # xyxy -> xywh (COCOæ ¼å¼)
            boxes_xywh = torch.zeros_like(pred_boxes)
            boxes_xywh[:, 0] = boxes_xyxy[:, 0]  # x
            boxes_xywh[:, 1] = boxes_xyxy[:, 1]  # y
            boxes_xywh[:, 2] = boxes_xyxy[:, 2] - boxes_xyxy[:, 0]  # w
            boxes_xywh[:, 3] = boxes_xyxy[:, 3] - boxes_xyxy[:, 1]  # h
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦é¢„æµ‹ï¼ˆä½¿ç”¨ä¼ å…¥çš„score_thresholdå‚æ•°ï¼‰
            keep = max_scores > score_threshold
            
            for score, label, box in zip(max_scores[keep], labels[keep], boxes_xywh[keep]):
                results.append({
                    'image_id': image_id,
                    'category_id': label.item(),
                    'bbox': box.cpu().tolist(),
                    'score': score.item(),
                })
    
    logger.info(f"ç”Ÿæˆäº† {len(results)} ä¸ªæ£€æµ‹ç»“æœ")
    
    if len(results) == 0:
        logger.warning("æ²¡æœ‰æ£€æµ‹ç»“æœï¼")
        return {}
    
    # ä½¿ç”¨COCO APIè¯„ä¼°
    logger.info("å¼€å§‹COCOè¯„ä¼°...")
    coco_dt = coco_gt.loadRes(results)
    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–å…³é”®æŒ‡æ ‡
    metrics = {
        'mAP': coco_eval.stats[0],
        'mAP_50': coco_eval.stats[1],
        'mAP_75': coco_eval.stats[2],
        'mAP_small': coco_eval.stats[3],
        'mAP_medium': coco_eval.stats[4],
        'mAP_large': coco_eval.stats[5],
    }
    
    return metrics


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°DETRæ¨¡å‹")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="æ¨¡å‹checkpointè·¯å¾„",
    )
    parser.add_argument(
        "--eval-set",
        type=str,
        default="val",
        choices=["train", "val", "test"],
        help="è¯„ä¼°æ•°æ®é›†",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ç»“æœè¾“å‡ºè·¯å¾„",
    )
    parser.add_argument(
        "--score-threshold",
        type=float,
        default=0.05,
        help="æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.05ï¼‰",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(config['device']['type'] if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger('eval')
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“¦ æ„å»ºæ•°æ®åŠ è½½å™¨")
    dataloader = build_dataloader(config, args.eval_set)
    
    # åŠ è½½COCO ground truth
    root = Path(config['dataset']['root_dir'])
    ann_file = root / config['dataset'][f'{args.eval_set}_ann']
    coco_gt = COCO(str(ann_file))
    
    # æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹")
    model = build_detr_model(config)
    model = model.to(device)
    
    # åŠ è½½checkpoint
    print(f"\nğŸ“‚ åŠ è½½checkpoint: {args.checkpoint}")
    load_checkpoint(args.checkpoint, model, device=str(device))
    
    # è¯„ä¼°
    print("\nğŸ¯ å¼€å§‹è¯„ä¼°")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {args.score_threshold}")
    print("="*60)
    metrics = evaluate(model, dataloader, device, coco_gt, logger, args.score_threshold)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("="*60)
    for k, v in metrics.items():
        print(f"  {k}: {v:.4f}")
    print("="*60)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == '__main__':
    main()
