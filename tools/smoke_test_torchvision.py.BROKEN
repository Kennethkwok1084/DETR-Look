#!/usr/bin/env python3
"""
å¿«é€Ÿå†’çƒŸæµ‹è¯•è„šæœ¬ï¼ˆTorchvision DETRï¼‰
éªŒè¯è®­ç»ƒé“¾è·¯ï¼šdataloader -> forward -> backward -> save
"""

import argparse
from pathlib import Path

import torch
from torch.utils.data import DataLoader, Subset
from pycocotools.coco import COCO

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„ç»„ä»¶
import sys
sys.path.insert(0, str(Path(__file__).parent))
from train_detr_torchvision import CocoDetrDataset, collate_fn, build_model


def smoke_test(
    train_img: str,
    train_ann: str,
    num_classes: int = 3,
    batch_size: int = 4,
    max_iters: int = 10,
    device: str = "cuda",
):
    """å†’çƒŸæµ‹è¯•ï¼š10 æ­¥è®­ç»ƒéªŒè¯é“¾è·¯"""
    
    print("=" * 80)
    print("ğŸ”¥ Torchvision DETR å†’çƒŸæµ‹è¯•")
    print("=" * 80)
    
    # è®¾å¤‡
    device = torch.device(device)
    
    # ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision("high")
    
    # æ•°æ®é›†ï¼ˆåªç”¨ 100 å¼ å›¾ï¼‰
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = CocoDetrDataset(train_img, train_ann, min_size=800, max_size=1333)
    subset = Subset(dataset, range(min(100, len(dataset))))
    
    loader = DataLoader(
        subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    print(f"  âœ“ æ•°æ®é›†å¤§å°: {len(subset)}")
    print(f"  âœ“ Batch size: {batch_size}")
    
    # æ¨¡å‹
    print("ğŸ“¦ æ„å»ºæ¨¡å‹...")
    model = build_model(num_classes, pretrained_backbone=False)
    model.to(device)
    model.train()
    print(f"  âœ“ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # è®­ç»ƒå‡ æ­¥
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("-" * 80)
    
    losses = []
    
    for step, (images, targets) in enumerate(loader, start=1):
        images = [img.to(device, non_blocking=True) for img in images]
        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]
        
        optimizer.zero_grad(set_to_none=True)
        
        # Forward
        loss_dict = model(images, targets)
        loss = sum(loss_dict.values())
        
        # Backward
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        print(f"Step [{step}/{max_iters}] Loss: {loss.item():.4f}")
        
        if step >= max_iters:
            break
    
    # éªŒè¯ loss æ˜¯å¦ä¸‹é™ï¼ˆè¶‹åŠ¿ï¼‰
    avg_loss_first_half = sum(losses[:len(losses)//2]) / (len(losses)//2 + 1e-8)
    avg_loss_second_half = sum(losses[len(losses)//2:]) / (len(losses)//2 + 1e-8)
    
    print("-" * 80)
    print(f"ğŸ“Š å‰åŠæ®µå¹³å‡ Loss: {avg_loss_first_half:.4f}")
    print(f"ğŸ“Š ååŠæ®µå¹³å‡ Loss: {avg_loss_second_half:.4f}")
    
    # ä¿å­˜æµ‹è¯•
    print("\nğŸ’¾ æµ‹è¯• checkpoint ä¿å­˜...")
    output_dir = Path("outputs/smoke_test_torchvision")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "losses": losses,
    }
    torch.save(checkpoint, output_dir / "smoke_test.pth")
    print(f"  âœ“ ä¿å­˜æˆåŠŸ: {output_dir / 'smoke_test.pth'}")
    
    # åŠ è½½æµ‹è¯•
    print("ğŸ“¥ æµ‹è¯• checkpoint åŠ è½½...")
    loaded = torch.load(output_dir / "smoke_test.pth")
    model.load_state_dict(loaded["model"])
    print("  âœ“ åŠ è½½æˆåŠŸ")
    
    # æ¨ç†æµ‹è¯•
    print("\nğŸ”® æµ‹è¯•æ¨ç†æ¨¡å¼...")
    model.eval()
    with torch.no_grad():
        images_test = [images[0].to(device)]
        outputs = model(images_test)
        print(f"  âœ“ è¾“å‡ºæ ¼å¼: {list(outputs[0].keys())}")
        print(f"  âœ“ æ£€æµ‹æ¡†æ•°é‡: {len(outputs[0]['boxes'])}")
    
    print("\n" + "=" * 80)
    print("âœ… å†’çƒŸæµ‹è¯•é€šè¿‡ï¼")
    print("=" * 80)
    print("\nå¯ä»¥å¼€å§‹å®Œæ•´è®­ç»ƒï¼š")
    print(f"  python tools/train_detr_torchvision.py \\")
    print(f"    --train-img {train_img} \\")
    print(f"    --train-ann {train_ann} \\")
    print(f"    --batch-size 16 --num-workers 12 --amp")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-img", type=str, 
                       default="data/traffic_coco/bdd100k_det/images/train")
    parser.add_argument("--train-ann", type=str,
                       default="data/traffic_coco/bdd100k_det/annotations/instances_train.json")
    parser.add_argument("--num-classes", type=int, default=3)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--max-iters", type=int, default=10)
    parser.add_argument("--device", type=str, default="cuda")
    
    args = parser.parse_args()
    
    smoke_test(
        args.train_img,
        args.train_ann,
        args.num_classes,
        args.batch_size,
        args.max_iters,
        args.device,
    )


if __name__ == "__main__":
    main()
