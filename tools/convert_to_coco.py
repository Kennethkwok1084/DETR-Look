#!/usr/bin/env python3
"""
BDD100K åˆ° COCO æ ¼å¼è½¬æ¢è„šæœ¬

é…ç½®èŒè´£åˆ†å·¥ï¼ˆé…ç½®é©±åŠ¨è®¾è®¡ï¼‰ï¼š
- ç±»åˆ«æ˜ å°„å®šä¹‰ï¼šä» configs/classes.yaml è¯»å–ï¼ˆé…ç½®é©±åŠ¨ï¼‰
  Â· COARSE_CLASSES: ç²—ç²’åº¦ç±»åˆ«å®šä¹‰ï¼ˆID -> åç§°ï¼‰
  Â· BDD100K_MAPPING: BDD100KåŸå§‹ç±»åˆ«åˆ°ç²—ç²’åº¦ç±»åˆ«çš„æ˜ å°„
  Â· MAPPING_OPTIONS: å¯é€‰æ˜ å°„å¼€å…³ï¼ˆbike/motorå¹¶å…¥ï¼‰
- æ˜ å°„è®°å½•ï¼šè½¬æ¢åè¾“å‡º mapping.jsonï¼Œè®°å½•é…ç½®å¿«ç…§å’Œç»Ÿè®¡ä¿¡æ¯
- éªŒè¯æœºåˆ¶ï¼šå¯åŠ¨æ—¶æ ¡éªŒé…ç½®åˆæ³•æ€§ï¼ˆIDè¿ç»­æ€§ã€æ˜ å°„å®Œæ•´æ€§ï¼‰

æ³¨æ„ï¼šä¿®æ”¹ç±»åˆ«æ˜ å°„åªéœ€ä¿®æ”¹ configs/classes.yaml
"""

import argparse
import json
import os
import shutil
import sys
import xml.etree.ElementTree as ET
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import yaml
from tqdm import tqdm


def load_classes_config(config_path: str) -> Dict:
    """åŠ è½½å¹¶éªŒè¯ç±»åˆ«é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # éªŒè¯å¿…éœ€å­—æ®µ
    required_fields = ['COARSE_CLASSES', 'MAPPING_OPTIONS']
    for field in required_fields:
        if field not in config:
            raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
    
    return config


def validate_common_config(config: Dict) -> Tuple[Dict[str, int], Dict[str, bool]]:
    """
    éªŒè¯é€šç”¨é…ç½®åˆæ³•æ€§å¹¶è¿”å› coarse_id

    Returns:
        coarse_id: {class_name: class_id} æ˜ å°„
        mapping_options: æ˜ å°„é€‰é¡¹
    """
    coarse_classes = config['COARSE_CLASSES']
    mapping_options = config['MAPPING_OPTIONS']

    # éªŒè¯1: COARSE_CLASSESçš„IDå¿…é¡»ä»0å¼€å§‹è¿ç»­
    class_ids = sorted(coarse_classes.keys())
    expected_ids = list(range(len(class_ids)))
    if class_ids != expected_ids:
        raise ValueError(
            f"COARSE_CLASSESçš„IDå¿…é¡»ä»0å¼€å§‹è¿ç»­\n"
            f"  æœŸæœ›: {expected_ids}\n"
            f"  å®é™…: {class_ids}"
        )

    # ç”Ÿæˆ coarse_id æ˜ å°„ {name: id}
    coarse_id = {name: idx for idx, name in coarse_classes.items()}

    return coarse_id, mapping_options


def validate_mapping(mapping: Dict[str, str], coarse_id: Dict[str, int], name: str) -> Dict[str, str]:
    """éªŒè¯æ˜ å°„å­—å…¸çš„valueæ˜¯å¦å­˜åœ¨äºCOARSE_CLASSESä¸­"""
    invalid_mappings = []
    for original_class, coarse_class in mapping.items():
        if coarse_class not in coarse_id:
            invalid_mappings.append(f"  '{original_class}' -> '{coarse_class}' (ä¸å­˜åœ¨)")

    if invalid_mappings:
        raise ValueError(
            f"{name}åŒ…å«æ— æ•ˆçš„ç²—ç²’åº¦ç±»åˆ«:\n" + "\n".join(invalid_mappings)
        )
    return dict(mapping)


def build_bdd_class_map(
    config: Dict,
    coarse_id: Dict[str, int],
    mapping_options: Dict[str, bool],
) -> Dict[str, str]:
    """æ„å»ºBDD100Kç±»åˆ«æ˜ å°„"""
    bdd_mapping = config.get('BDD100K_MAPPING')
    if not bdd_mapping:
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘ BDD100K_MAPPING")
    bdd_class_map = validate_mapping(bdd_mapping, coarse_id, "BDD100K_MAPPING")

    # æ ¹æ®MAPPING_OPTIONSåŠ¨æ€æ·»åŠ bike/motoræ˜ å°„
    if mapping_options.get('include_bike', False):
        bike_target = mapping_options.get('bike_target', 'vehicle')
        if bike_target not in coarse_id:
            raise ValueError(
                f"MAPPING_OPTIONS.bike_target='{bike_target}' ä¸å­˜åœ¨äºCOARSE_CLASSESä¸­"
            )
        bdd_class_map['bike'] = bike_target

    if mapping_options.get('include_motor', False):
        motor_target = mapping_options.get('motor_target', 'vehicle')
        if motor_target not in coarse_id:
            raise ValueError(
                f"MAPPING_OPTIONS.motor_target='{motor_target}' ä¸å­˜åœ¨äºCOARSE_CLASSESä¸­"
            )
        bdd_class_map['motor'] = motor_target

    return bdd_class_map


def build_cctsdb_class_map(config: Dict, coarse_id: Dict[str, int]) -> Dict[str, str]:
    """æ„å»ºCCTSDBç±»åˆ«æ˜ å°„"""
    cctsdb_mapping = config.get('CCTSDB_MAPPING')
    if not cctsdb_mapping:
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘ CCTSDB_MAPPING")
    return validate_mapping(cctsdb_mapping, coarse_id, "CCTSDB_MAPPING")


def resolve_tt100k_target(config: Dict, coarse_id: Dict[str, int]) -> str:
    """è§£æTT100Kçš„ç›®æ ‡ç²—ç²’åº¦ç±»åˆ«"""
    target = config.get('TT100K_TARGET', 'traffic_sign')
    if target not in coarse_id:
        raise ValueError(f"TT100K_TARGET='{target}' ä¸å­˜åœ¨äºCOARSE_CLASSESä¸­")
    return target


def print_config_summary(coarse_id: Dict[str, int], class_map: Dict[str, str]) -> None:
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“‹ é…ç½®æ‘˜è¦")
    print("="*60)
    
    print("\nç²—ç²’åº¦ç±»åˆ«:")
    for name, idx in sorted(coarse_id.items(), key=lambda x: x[1]):
        print(f"  [{idx}] {name}")
    
    print(f"\næ˜ å°„è§„åˆ™ ({len(class_map)} ä¸ª):")
    for original, coarse in sorted(class_map.items()):
        class_id = coarse_id[coarse]
        print(f"  '{original}' -> '{coarse}' (ID: {class_id})")
    
    print("="*60 + "\n")


def print_tt100k_summary(coarse_id: Dict[str, int], target: str) -> None:
    """æ‰“å°TT100Ké…ç½®æ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“‹ é…ç½®æ‘˜è¦")
    print("="*60)

    print("\nç²—ç²’åº¦ç±»åˆ«:")
    for name, idx in sorted(coarse_id.items(), key=lambda x: x[1]):
        print(f"  [{idx}] {name}")

    print(f"\nTT100Kç›®æ ‡ç±»åˆ«: '{target}' (ID: {coarse_id[target]})")
    print("="*60 + "\n")


def resolve_image_name(img_src_dir: Path, name: str) -> str:
    """æ ¹æ®å®é™…æ–‡ä»¶è¡¥å…¨å›¾ç‰‡åç¼€ï¼ˆä¼˜å…ˆjpgï¼Œå…¶æ¬¡pngï¼‰"""
    candidate = img_src_dir / name
    if candidate.exists():
        return name
    if Path(name).suffix:
        return name
    for ext in [".jpg", ".png"]:
        candidate = img_src_dir / f"{name}{ext}"
        if candidate.exists():
            return f"{name}{ext}"
    return name


def load_per_image_labels(label_dir: Path) -> List[Dict]:
    """åŠ è½½å•å›¾JSONæ ‡æ³¨å¹¶è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
    json_files = sorted(label_dir.glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"æ ‡æ³¨ç›®å½•ä¸ºç©º: {label_dir}")

    annotations: List[Dict] = []
    multi_frame = 0
    for label_path in tqdm(json_files, desc=f"Loading {label_dir.name} labels"):
        with open(label_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        name = data.get("name", label_path.stem)
        frames = data.get("frames", [])
        if len(frames) > 1:
            multi_frame += 1

        labels: List[Dict] = []
        if frames:
            # å•å›¾æ ‡æ³¨é€šå¸¸åªæœ‰ä¸€å¸§ï¼Œé»˜è®¤å–ç¬¬ä¸€å¸§
            for obj in frames[0].get("objects", []):
                if "box2d" not in obj:
                    continue
                labels.append({
                    "category": obj.get("category"),
                    "box2d": obj.get("box2d"),
                })

        annotations.append({
            "name": name,
            "labels": labels,
        })

    if multi_frame > 0:
        print(f"âš ï¸  æ£€æµ‹åˆ° {multi_frame} ä¸ªå¤šå¸§æ ‡æ³¨æ–‡ä»¶ï¼Œå·²é»˜è®¤ä½¿ç”¨é¦–å¸§")

    return annotations


def get_image_size(image_path: Path) -> Tuple[int, int]:
    """è¯»å–å›¾ç‰‡å°ºå¯¸ï¼ˆä¾èµ–opencv-pythonï¼‰"""
    try:
        import cv2
    except ImportError as exc:
        raise RuntimeError("è¯»å–å›¾ç‰‡å°ºå¯¸éœ€è¦å®‰è£… opencv-python") from exc

    image = cv2.imread(str(image_path))
    if image is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
    height, width = image.shape[:2]
    return width, height


def parse_cctsdb_xml(xml_path: Path) -> Dict:
    """è§£æCCTSDB XMLæ ‡æ³¨"""
    tree = ET.parse(xml_path)
    root = tree.getroot()

    filename = root.findtext("filename") or xml_path.stem
    size_node = root.find("size")
    width = int(size_node.findtext("width")) if size_node is not None else 0
    height = int(size_node.findtext("height")) if size_node is not None else 0

    labels = []
    for obj in root.findall("object"):
        name = obj.findtext("name")
        box = obj.find("bndbox")
        if box is None:
            continue
        labels.append({
            "category": name,
            "box2d": {
                "x1": float(box.findtext("xmin")),
                "y1": float(box.findtext("ymin")),
                "x2": float(box.findtext("xmax")),
                "y2": float(box.findtext("ymax")),
            },
        })

    return {
        "name": filename,
        "width": width,
        "height": height,
        "labels": labels,
    }

def convert_bdd_to_coco(
    src_dir: str,
    dst_dir: str,
    split: str,
    coarse_id: Dict[str, int],
    class_map: Dict[str, str],
    min_area: float = 0.0,
) -> Tuple[Dict, Dict]:
    """
    å°†BDD100Kæ ¼å¼è½¬æ¢ä¸ºCOCOæ ¼å¼
    
    Args:
        src_dir: BDD100Kæ•°æ®é›†æ ¹ç›®å½•
        dst_dir: è¾“å‡ºCOCOæ ¼å¼æ•°æ®é›†ç›®å½•
        split: æ•°æ®é›†åˆ’åˆ† (train/val/test)
        coarse_id: ç²—ç²’åº¦ç±»åˆ«IDæ˜ å°„ {class_name: class_id}
        class_map: BDDç±»åˆ«æ˜ å°„ {original_class: coarse_class}
        min_area: æœ€å°bboxé¢ç§¯è¿‡æ»¤é˜ˆå€¼
        
    Returns:
        coco_dict: COCOæ ¼å¼çš„æ ‡æ³¨å­—å…¸
        stats: è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
    """
    # è·¯å¾„è®¾ç½®ï¼ˆå…¼å®¹å¤šç§å›¾åƒç›®å½•ç»“æ„ï¼‰
    img_src_candidates = [
        Path(src_dir) / "images" / "100k" / split,
        Path(src_dir) / "images" / split,
        Path(src_dir) / split,
    ]
    img_src_dir = next((p for p in img_src_candidates if p.exists()), img_src_candidates[0])
    
    # BDD100Kæ ‡æ³¨æ–‡ä»¶è·¯å¾„fallbackï¼ˆæ”¯æŒå¤šç§å®˜æ–¹æ ¼å¼ï¼‰
    label_candidates = [
        Path(src_dir) / "labels" / f"det_{split}.json",              # æ—§ç‰ˆ: det_train.json
        Path(src_dir) / "labels" / "det_20" / f"det_{split}.json",   # æ–°ç‰ˆå­ç›®å½•: det_20/det_train.json
        Path(src_dir) / "labels" / f"det_20_{split}.json",           # æ‰å¹³å‘½å: det_20_train.json
    ]

    # å•å›¾JSONæ ‡æ³¨è·¯å¾„ï¼ˆé€å›¾æ–‡ä»¶ï¼‰
    label_dir_candidates = [
        Path(src_dir) / "labels" / "bdd100k" / split,
        Path(src_dir) / "labels" / "bd100k" / split,
    ]

    label_src_file = next((p for p in label_candidates if p.exists()), None)
    label_src_dir = next((p for p in label_dir_candidates if p.exists()), None)

    img_dst_dir = Path(dst_dir) / "images" / split
    ann_dst_dir = Path(dst_dir) / "annotations"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    img_dst_dir.mkdir(parents=True, exist_ok=True)
    ann_dst_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if label_src_file is None and label_src_dir is None:
        raise FileNotFoundError(
            f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {label_candidates[0]}ï¼Œä¸”æœªæ‰¾åˆ°é€å›¾æ ‡æ³¨ç›®å½•"
        )
    if not img_src_dir.exists():
        raise FileNotFoundError(
            f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_src_candidates[0]}ï¼Œä¸”æœªæ‰¾åˆ°å¯ç”¨çš„å›¾åƒç›®å½•ç»“æ„"
        )
    
    # åŠ è½½BDD100Kæ ‡æ³¨
    if label_src_file is not None:
        print(f"ğŸ“‚ åŠ è½½ {split} é›†æ ‡æ³¨: {label_src_file}")
        with open(label_src_file, 'r') as f:
            bdd_annotations = json.load(f)
    else:
        print(f"ğŸ“‚ åŠ è½½ {split} é›†é€å›¾æ ‡æ³¨: {label_src_dir}")
        bdd_annotations = load_per_image_labels(label_src_dir)
    
    # åˆå§‹åŒ–COCOæ ¼å¼
    coco_dict = {
        "images": [],
        "annotations": [],
        "categories": [],
    }
    
    # åˆ›å»ºç±»åˆ«åˆ—è¡¨
    for class_name, class_id in coarse_id.items():
        coco_dict["categories"].append({
            "id": class_id,
            "name": class_name,
            "supercategory": "traffic",
        })
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_images": 0,
        "total_annotations": 0,
        "class_counts": defaultdict(int),
        "original_class_counts": defaultdict(int),
        "filtered_annotations": 0,
        "unmapped_classes": set(),
    }
    
    annotation_id = 0
    
    # å¤„ç†æ¯å¼ å›¾åƒ
    print(f"ğŸ”„ è½¬æ¢ {split} é›†...")
    for img_idx, img_data in enumerate(tqdm(bdd_annotations, desc=f"Processing {split}")):
        img_name = resolve_image_name(img_src_dir, img_data["name"])
        
        # æ·»åŠ å›¾åƒä¿¡æ¯
        coco_dict["images"].append({
            "id": img_idx,
            "file_name": img_name,
            "width": 1280,  # BDD100Ké»˜è®¤åˆ†è¾¨ç‡
            "height": 720,
        })
        stats["total_images"] += 1
        
        # å¤åˆ¶å›¾åƒæ–‡ä»¶
        src_img = img_src_dir / img_name
        dst_img = img_dst_dir / img_name
        if src_img.exists() and not dst_img.exists():
            shutil.copy2(src_img, dst_img)
        
        # å¤„ç†æ ‡æ³¨
        if "labels" not in img_data:
            continue
            
        for label in img_data["labels"]:
            if "box2d" not in label:
                continue
            
            original_category = label["category"]
            stats["original_class_counts"][original_category] += 1
            
            # ç±»åˆ«æ˜ å°„
            if original_category not in class_map:
                stats["unmapped_classes"].add(original_category)
                continue
            
            coarse_category = class_map[original_category]
            category_id = coarse_id[coarse_category]
            
            # æå–bbox
            box = label["box2d"]
            x1, y1 = box["x1"], box["y1"]
            x2, y2 = box["x2"], box["y2"]
            w, h = x2 - x1, y2 - y1
            
            # é¢ç§¯è¿‡æ»¤
            area = w * h
            if area < min_area:
                stats["filtered_annotations"] += 1
                continue
            
            # æ·»åŠ æ ‡æ³¨
            coco_dict["annotations"].append({
                "id": annotation_id,
                "image_id": img_idx,
                "category_id": category_id,
                "bbox": [x1, y1, w, h],
                "area": area,
                "iscrowd": 0,
            })
            
            stats["total_annotations"] += 1
            stats["class_counts"][coarse_category] += 1
            annotation_id += 1
    
    # ä¿å­˜COCOæ ¼å¼æ ‡æ³¨
    ann_file = ann_dst_dir / f"instances_{split}.json"
    print(f"ğŸ’¾ ä¿å­˜æ ‡æ³¨æ–‡ä»¶: {ann_file}")
    with open(ann_file, 'w') as f:
        json.dump(coco_dict, f)
    
    return coco_dict, stats


def convert_cctsdb_to_coco(
    src_dir: str,
    dst_dir: str,
    split: str,
    coarse_id: Dict[str, int],
    class_map: Dict[str, str],
    min_area: float = 0.0,
) -> Tuple[Dict, Dict]:
    """å°†CCTSDB XMLæ ‡æ³¨è½¬æ¢ä¸ºCOCOæ ¼å¼"""
    img_src_candidates = [
        Path(src_dir) / "images" / split,
        Path(src_dir) / split,
    ]
    img_src_dir = next((p for p in img_src_candidates if p.exists()), img_src_candidates[0])

    label_dir_candidates = [
        Path(src_dir) / "labels" / "xml" / split,
        Path(src_dir) / "labels" / "xml",
        Path(src_dir) / "xml" / split,
        Path(src_dir) / "xml",
        Path(src_dir) / "labels" / split,
    ]
    label_src_dir = next((p for p in label_dir_candidates if p.exists()), None)

    img_dst_dir = Path(dst_dir) / "images" / split
    ann_dst_dir = Path(dst_dir) / "annotations"
    img_dst_dir.mkdir(parents=True, exist_ok=True)
    ann_dst_dir.mkdir(parents=True, exist_ok=True)

    if not img_src_dir.exists():
        raise FileNotFoundError(
            f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_src_candidates[0]}ï¼Œä¸”æœªæ‰¾åˆ°å¯ç”¨çš„å›¾åƒç›®å½•ç»“æ„"
        )
    if label_src_dir is None:
        raise FileNotFoundError("æœªæ‰¾åˆ°CCTSDB XMLæ ‡æ³¨ç›®å½•")

    image_files = {p.stem: p.name for p in img_src_dir.glob("*.*")}

    coco_dict = {
        "images": [],
        "annotations": [],
        "categories": [],
    }
    for class_name, class_id in coarse_id.items():
        coco_dict["categories"].append({
            "id": class_id,
            "name": class_name,
            "supercategory": "traffic",
        })

    stats = {
        "total_images": 0,
        "total_annotations": 0,
        "class_counts": defaultdict(int),
        "original_class_counts": defaultdict(int),
        "filtered_annotations": 0,
        "unmapped_classes": set(),
    }
    annotation_id = 0

    xml_files = sorted(label_src_dir.glob("*.xml"))
    if not xml_files:
        raise FileNotFoundError(f"æ ‡æ³¨ç›®å½•ä¸ºç©º: {label_src_dir}")

    print(f"ğŸ“‚ åŠ è½½ {split} é›†XMLæ ‡æ³¨: {label_src_dir}")
    for img_idx, xml_path in enumerate(tqdm(xml_files, desc=f"Processing {split}")):
        data = parse_cctsdb_xml(xml_path)
        img_name = resolve_image_name(img_src_dir, data["name"])
        if Path(img_name).stem not in image_files:
            continue

        width = data.get("width", 0)
        height = data.get("height", 0)
        if width == 0 or height == 0:
            width, height = get_image_size(img_src_dir / img_name)

        coco_dict["images"].append({
            "id": img_idx,
            "file_name": img_name,
            "width": width,
            "height": height,
        })
        stats["total_images"] += 1

        src_img = img_src_dir / img_name
        dst_img = img_dst_dir / img_name
        if src_img.exists() and not dst_img.exists():
            shutil.copy2(src_img, dst_img)

        for label in data.get("labels", []):
            original_category = label["category"]
            stats["original_class_counts"][original_category] += 1
            if original_category not in class_map:
                stats["unmapped_classes"].add(original_category)
                continue

            coarse_category = class_map[original_category]
            category_id = coarse_id[coarse_category]
            box = label["box2d"]
            x1, y1 = box["x1"], box["y1"]
            x2, y2 = box["x2"], box["y2"]
            w, h = x2 - x1, y2 - y1
            area = w * h
            if area < min_area:
                stats["filtered_annotations"] += 1
                continue

            coco_dict["annotations"].append({
                "id": annotation_id,
                "image_id": img_idx,
                "category_id": category_id,
                "bbox": [x1, y1, w, h],
                "area": area,
                "iscrowd": 0,
            })
            stats["total_annotations"] += 1
            stats["class_counts"][coarse_category] += 1
            annotation_id += 1

    ann_file = ann_dst_dir / f"instances_{split}.json"
    print(f"ğŸ’¾ ä¿å­˜æ ‡æ³¨æ–‡ä»¶: {ann_file}")
    with open(ann_file, 'w') as f:
        json.dump(coco_dict, f)

    return coco_dict, stats


def convert_tt100k_to_coco(
    src_dir: str,
    dst_dir: str,
    split: str,
    coarse_id: Dict[str, int],
    target_class: str,
    min_area: float = 0.0,
) -> Tuple[Dict, Dict]:
    """å°†TT100Kå®˜æ–¹æ ‡æ³¨è½¬æ¢ä¸ºCOCOæ ¼å¼"""
    ann_file = Path(src_dir) / "annotations_all.json"
    if not ann_file.exists():
        raise FileNotFoundError(f"TT100Kæ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {ann_file}")

    with open(ann_file, 'r', encoding='utf-8') as f:
        ann_data = json.load(f)

    imgs = ann_data.get("imgs", {})
    img_dst_dir = Path(dst_dir) / "images" / split
    ann_dst_dir = Path(dst_dir) / "annotations"
    img_dst_dir.mkdir(parents=True, exist_ok=True)
    ann_dst_dir.mkdir(parents=True, exist_ok=True)

    coco_dict = {
        "images": [],
        "annotations": [],
        "categories": [],
    }
    for class_name, class_id in coarse_id.items():
        coco_dict["categories"].append({
            "id": class_id,
            "name": class_name,
            "supercategory": "traffic",
        })

    stats = {
        "total_images": 0,
        "total_annotations": 0,
        "class_counts": defaultdict(int),
        "original_class_counts": defaultdict(int),
        "filtered_annotations": 0,
        "unmapped_classes": set(),
    }
    annotation_id = 0
    image_id = 0

    split_prefix = f"{split}/"
    # ä½¿ç”¨æ•°å€¼æ’åºç¡®ä¿ image_id æŒ‰å›¾åƒ ID é¡ºåºç”Ÿæˆï¼ˆè€Œéå­—ç¬¦ä¸²å­—å…¸åºï¼‰
    # æ·»åŠ å®‰å…¨å…œåº•ï¼šæ— æ³•è½¬ int æ—¶é€€å›å­—ç¬¦ä¸²æ’åº
    def safe_numeric_sort(item):
        try:
            return (0, int(item[0]))  # (ä¼˜å…ˆçº§, æ•°å€¼)
        except (ValueError, TypeError):
            return (1, item[0])  # (ä¼˜å…ˆçº§, å­—ç¬¦ä¸²) - éæ•°å­—keyæ”¾åé¢
    
    sorted_imgs = sorted(imgs.items(), key=safe_numeric_sort)
    for img_key, img_info in tqdm(sorted_imgs, desc=f"Processing {split}"):
        path = img_info.get("path", "")
        if not path.startswith(split_prefix):
            continue

        img_path = Path(src_dir) / path
        if not img_path.exists():
            continue

        width = img_info.get("width")
        height = img_info.get("height")
        if width is None or height is None:
            width, height = get_image_size(img_path)

        file_name = Path(path).name
        coco_dict["images"].append({
            "id": image_id,
            "file_name": file_name,
            "width": int(width),
            "height": int(height),
        })
        stats["total_images"] += 1

        dst_img = img_dst_dir / file_name
        if not dst_img.exists():
            shutil.copy2(img_path, dst_img)

        for obj in img_info.get("objects", []):
            bbox = obj.get("bbox")
            if not bbox:
                continue
            original_category = obj.get("category", "unknown")
            stats["original_class_counts"][original_category] += 1

            x1, y1 = float(bbox["xmin"]), float(bbox["ymin"])
            x2, y2 = float(bbox["xmax"]), float(bbox["ymax"])
            w, h = x2 - x1, y2 - y1
            area = w * h
            if area < min_area:
                stats["filtered_annotations"] += 1
                continue

            coco_dict["annotations"].append({
                "id": annotation_id,
                "image_id": image_id,
                "category_id": coarse_id[target_class],
                "bbox": [x1, y1, w, h],
                "area": area,
                "iscrowd": 0,
            })
            stats["total_annotations"] += 1
            stats["class_counts"][target_class] += 1
            annotation_id += 1
        
        image_id += 1

    ann_out = ann_dst_dir / f"instances_{split}.json"
    print(f"ğŸ’¾ ä¿å­˜æ ‡æ³¨æ–‡ä»¶: {ann_out}")
    with open(ann_out, 'w') as f:
        json.dump(coco_dict, f)

    return coco_dict, stats

def save_mapping_info(
    dst_dir: str,
    all_stats: Dict[str, Dict],
    coarse_id: Dict[str, int],
    class_map: Dict[str, str],
    config_path: str,
    config_content: Dict,
    mapping_key: str,
) -> None:
    """ä¿å­˜æ˜ å°„ä¿¡æ¯å’Œç»Ÿè®¡æ‘˜è¦"""
    # å°†é…ç½®è½¬ä¸ºYAMLå­—ç¬¦ä¸²ä¿å­˜ï¼Œé¿å…JSONåºåˆ—åŒ–æ—¶int keyå˜æˆå­—ç¬¦ä¸²
    config_yaml = yaml.dump(config_content, allow_unicode=True, sort_keys=False)
    
    mapping_info = {
        "class_mapping": {
            mapping_key: class_map,
            "coarse_to_id": coarse_id,
        },
        "statistics": {},
        "config_snapshot": {
            "path": config_path,
            "content_yaml": config_yaml,  # YAMLåŸæ–‡ä¿æŒå®Œæ•´ç»“æ„
            "content_dict": config_content,  # dictä¾¿äºç¨‹åºè¯»å–
        },
    }
    
    # æ±‡æ€»å„splitçš„ç»Ÿè®¡ä¿¡æ¯
    for split, stats in all_stats.items():
        mapping_info["statistics"][split] = {
            "total_images": stats["total_images"],
            "total_annotations": stats["total_annotations"],
            "filtered_annotations": stats["filtered_annotations"],
            "class_counts": dict(stats["class_counts"]),
            "original_class_counts": dict(stats["original_class_counts"]),
            "unmapped_classes": list(stats["unmapped_classes"]),
        }
    
    # ä¿å­˜mapping.json
    mapping_file = Path(dst_dir) / "mapping.json"
    print(f"\nğŸ“‹ ä¿å­˜æ˜ å°„ä¿¡æ¯: {mapping_file}")
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping_info, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š è½¬æ¢æ‘˜è¦")
    print("="*60)
    
    for split, stats in all_stats.items():
        print(f"\nã€{split.upper()} é›†ã€‘")
        print(f"  æ€»å›¾åƒæ•°: {stats['total_images']:,}")
        print(f"  æ€»æ ‡æ³¨æ•°: {stats['total_annotations']:,}")
        print(f"  è¿‡æ»¤æ ‡æ³¨æ•°: {stats['filtered_annotations']:,}")
        print(f"  ç±»åˆ«åˆ†å¸ƒ:")
        for class_name, count in sorted(stats['class_counts'].items()):
            class_id = coarse_id[class_name]
            print(f"    [{class_id}] {class_name}: {count:,}")
        
        if stats['unmapped_classes']:
            print(f"  âš ï¸  æœªæ˜ å°„ç±»åˆ«: {', '.join(sorted(stats['unmapped_classes']))}")
    
    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(
        description="å°†BDD100Kæ•°æ®é›†è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼Œæ”¯æŒç²—ç²’åº¦ç±»åˆ«æ˜ å°„"
    )
    parser.add_argument(
        "--src",
        type=str,
        required=True,
        help="BDD100Kæ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å«imageså’Œlabelsç›®å½•ï¼‰",
    )
    parser.add_argument(
        "--dst",
        type=str,
        required=True,
        help="COCOæ ¼å¼è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--split-train",
        type=str,
        default="train",
        help="è®­ç»ƒé›†åç§°ï¼ˆé»˜è®¤: trainï¼‰",
    )
    parser.add_argument(
        "--split-val",
        type=str,
        default="val",
        help="éªŒè¯é›†åç§°ï¼ˆé»˜è®¤: valï¼‰",
    )
    parser.add_argument(
        "--split-test",
        type=str,
        default="test",
        help="æµ‹è¯•é›†åç§°ï¼ˆé»˜è®¤: testï¼‰",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/classes.yaml",
        help="ç±»åˆ«é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--min-area",
        type=float,
        default=None,
        help="æœ€å°bboxé¢ç§¯é˜ˆå€¼ï¼ˆåƒç´ ï¼‰ï¼ŒæœªæŒ‡å®šæ—¶ä»é…ç½®æ–‡ä»¶è¯»å–",
    )
    parser.add_argument(
        "--splits",
        nargs="+",
        default=["train", "val"],
        choices=["train", "val", "test"],
        help="è¦è½¬æ¢çš„æ•°æ®é›†åˆ’åˆ†",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="bdd100k",
        choices=["bdd100k", "cctsdb", "tt100k"],
        help="æ•°æ®é›†ç±»å‹",
    )
    
    args = parser.parse_args()
    
    # åŠ è½½å¹¶éªŒè¯é…ç½®
    print(f"ğŸ“– åŠ è½½ç±»åˆ«é…ç½®: {args.config}")
    try:
        config = load_classes_config(args.config)
        coarse_id, mapping_options = validate_common_config(config)
    except (FileNotFoundError, ValueError) as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # ä»é…ç½®è¯»å–min_areaï¼ˆå¦‚æœå‘½ä»¤è¡Œæœªæ˜¾å¼æŒ‡å®šï¼‰
    if args.min_area is None:
        args.min_area = mapping_options.get('min_area', 0.0)
    # å¦åˆ™ä½¿ç”¨å‘½ä»¤è¡Œå€¼ï¼ˆåŒ…æ‹¬æ˜¾å¼æŒ‡å®šçš„ 0.0ï¼‰
    
    # æ‰“å°é…ç½®æ‘˜è¦
    if args.dataset == "bdd100k":
        class_map = build_bdd_class_map(config, coarse_id, mapping_options)
        print_config_summary(coarse_id, class_map)
        converter = convert_bdd_to_coco
        mapping_key = "bdd100k_to_coarse"
        target_class = None
    elif args.dataset == "cctsdb":
        class_map = build_cctsdb_class_map(config, coarse_id)
        print_config_summary(coarse_id, class_map)
        converter = convert_cctsdb_to_coco
        mapping_key = "cctsdb_to_coarse"
        target_class = None
    else:
        target_class = resolve_tt100k_target(config, coarse_id)
        class_map = {"__all__": target_class}
        print_tt100k_summary(coarse_id, target_class)
        converter = convert_tt100k_to_coco
        mapping_key = "tt100k_to_coarse"
    
    # æ‰“å°è½¬æ¢ä¿¡æ¯
    print("="*60)
    print(f"ğŸš€ {args.dataset.upper()} â†’ COCO è½¬æ¢å·¥å…·")
    print("="*60)
    print(f"æºç›®å½•: {args.src}")
    print(f"ç›®æ ‡ç›®å½•: {args.dst}")
    print(f"æœ€å°é¢ç§¯: {args.min_area} åƒç´ Â²")
    print(f"è½¬æ¢åˆ’åˆ†: {', '.join(args.splits)}")
    print("="*60 + "\n")
    
    # è½¬æ¢å„ä¸ªsplit
    all_stats = {}
    split_map = {
        "train": args.split_train,
        "val": args.split_val,
        "test": args.split_test,
    }
    
    for split_key in args.splits:
        split_name = split_map[split_key]
        try:
            if args.dataset == "tt100k":
                _, stats = converter(
                    src_dir=args.src,
                    dst_dir=args.dst,
                    split=split_name,
                    coarse_id=coarse_id,
                    target_class=target_class,
                    min_area=args.min_area,
                )
            else:
                _, stats = converter(
                    src_dir=args.src,
                    dst_dir=args.dst,
                    split=split_name,
                    coarse_id=coarse_id,
                    class_map=class_map,
                    min_area=args.min_area,
                )
            all_stats[split_name] = stats
        except FileNotFoundError as e:
            print(f"âš ï¸  è·³è¿‡ {split_name} é›†: {e}")
            continue
    
    # ä¿å­˜æ˜ å°„ä¿¡æ¯å’Œæ‰“å°æ‘˜è¦
    if all_stats:
        save_mapping_info(
            args.dst,
            all_stats,
            coarse_id,
            class_map,
            args.config,
            config,
            mapping_key,
        )
        print(f"\nâœ… è½¬æ¢å®Œæˆï¼è¾“å‡ºç›®å½•: {args.dst}")
    else:
        print("\nâŒ æœªæˆåŠŸè½¬æ¢ä»»ä½•æ•°æ®é›†")


if __name__ == "__main__":
    main()
